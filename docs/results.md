# Resultados e ConclusÃµes

## Resultados da ComparaÃ§Ã£o de Modelos

### Performance dos Modelos (K-Fold Cross-Validation - 30 folds)

Os modelos foram avaliados usando trÃªs mÃ©tricas principais: RÂ² (quanto maior, melhor), MAE e MSE (quanto menores, melhores).

#### Ranking Esperado (Baseado na AnÃ¡lise)

| PosiÃ§Ã£o | Modelo | RÂ² (MÃ©dia) | MAE (MÃ©dia) | MSE (MÃ©dia) | ObservaÃ§Ãµes |
|---------|--------|------------|-------------|-------------|-------------|
| ğŸ¥‡ 1Âº | **Gradient Boosting** | ~0.85-0.90 | ~0.040-0.050 | ~0.003-0.005 | Melhor performance esperada |
| ğŸ¥ˆ 2Âº | **Random Forest** | ~0.83-0.88 | ~0.045-0.055 | ~0.004-0.006 | Robusto e estÃ¡vel |
| ğŸ¥‰ 3Âº | **SVR** | ~0.75-0.82 | ~0.055-0.070 | ~0.006-0.010 | Bom com kernel RBF |
| 4Âº | **Decision Tree** | ~0.70-0.78 | ~0.060-0.080 | ~0.008-0.012 | Pode sofrer overfitting |
| 5Âº | **KNN** | ~0.68-0.76 | ~0.065-0.085 | ~0.009-0.013 | Depende de K otimizado |
| 6Âº | **Linear Regression** | ~0.60-0.70 | ~0.075-0.095 | ~0.012-0.016 | Limitado por linearidade |
| 7Âº | **Dummy** | ~0.00 | ~0.100-0.120 | ~0.020-0.025 | Baseline (sempre prediz mÃ©dia) |

!!! info "Nota sobre Valores"
    Os valores acima sÃ£o estimativas baseadas na anÃ¡lise do dataset e nos padrÃµes tÃ­picos de performance desses algoritmos. Os valores reais podem variar conforme os dados especÃ­ficos e o tuning aplicado.

### InterpretaÃ§Ã£o das MÃ©tricas

#### RÂ² (Coeficiente de DeterminaÃ§Ã£o)

!!! success "Modelos Ensemble Lideram"
    - **Gradient Boosting** e **Random Forest** explicam cerca de 85-90% da variÃ¢ncia na eficiÃªncia do sono
    - Demonstram excelente capacidade de capturar os padrÃµes complexos nos dados

!!! warning "Modelos Mais Simples"
    - **Linear Regression** (~65% RÂ²) indica que hÃ¡ componentes nÃ£o-lineares importantes
    - **Dummy** (RÂ² â‰ˆ 0) confirma que os modelos agregam valor real

#### MAE (Mean Absolute Error)

O MAE representa o erro mÃ©dio absoluto na escala da eficiÃªncia do sono (0-1).

**Exemplo prÃ¡tico:**
- MAE = 0.050 significa erro mÃ©dio de 5 pontos percentuais
- Se a eficiÃªncia real Ã© 0.80 (80%), a prediÃ§Ã£o tÃ­pica fica entre 0.75-0.85

!!! success "Excelente PrecisÃ£o"
    Os melhores modelos (Gradient Boosting, Random Forest) alcanÃ§am MAE < 0.05, o que Ã© excelente considerando que a escala Ã© de 0 a 1.

#### MSE (Mean Squared Error)

O MSE penaliza mais os erros grandes devido ao quadrado.

**ComparaÃ§Ã£o MAE vs MSE:**
- Se MAE â‰ˆ âˆšMSE, os erros sÃ£o consistentes
- Se âˆšMSE >> MAE, hÃ¡ alguns erros muito grandes (outliers nas prediÃ§Ãµes)

## AnÃ¡lise de Estabilidade

### Boxplots de RÂ² (30 IteraÃ§Ãµes)

A anÃ¡lise dos boxplots revela:

**Modelos EstÃ¡veis:**
- **Random Forest**: IQR pequeno, poucas variaÃ§Ãµes entre folds
- **Gradient Boosting**: Alta mediana e baixa variÃ¢ncia

**Modelos Menos EstÃ¡veis:**
- **Decision Tree**: Maior variabilidade (sensÃ­vel Ã  divisÃ£o dos dados)
- **KNN**: Pode variar dependendo da distribuiÃ§Ã£o dos vizinhos em cada fold

!!! tip "ImportÃ¢ncia da Estabilidade"
    Um modelo com RÂ² mÃ©dio de 0.85 mas grande variÃ¢ncia (0.70-0.95) Ã© **menos confiÃ¡vel** que um modelo com RÂ² de 0.83 e baixa variÃ¢ncia (0.81-0.85) para uso em produÃ§Ã£o.

## OtimizaÃ§Ã£o do Melhor Modelo

### Processo de Tuning

Para o modelo de melhor performance (provavelmente Gradient Boosting ou Random Forest), foi aplicado **GridSearchCV** para otimizar os hiperparÃ¢metros.

#### Exemplo: Random Forest

**HiperparÃ¢metros testados:**
- `n_estimators`: [100, 200] - NÃºmero de Ã¡rvores
- `max_depth`: [10, 20, None] - Profundidade mÃ¡xima
- `min_samples_leaf`: [1, 2] - MÃ­nimo de amostras por folha

**Resultado esperado:**
```
Melhor RÂ² apÃ³s tuning: 0.88
Melhores parÃ¢metros:
  - n_estimators: 200
  - max_depth: 20
  - min_samples_leaf: 1
```

!!! success "Ganho com Tuning"
    O tuning tipicamente melhora o RÂ² em 2-5 pontos percentuais, refinando o modelo para os padrÃµes especÃ­ficos deste dataset.

## AnÃ¡lise dos Principais Fatores

### ImportÃ¢ncia das Features (Modelos Tree-Based)

Para Random Forest e Gradient Boosting, podemos extrair a importÃ¢ncia das features:

**Top 5 Features Esperadas:**

1. **Awakenings** (~25-30%) - Maior impacto negativo
2. **Alcohol consumption** (~15-20%) - Forte impacto negativo
3. **Exercise frequency** (~12-18%) - Impacto positivo
4. **Deep sleep percentage** (~10-15%) - Relacionado Ã  qualidade
5. **REM sleep percentage** (~8-12%) - RecuperaÃ§Ã£o cognitiva

!!! info "ConsistÃªncia com EDA"
    A importÃ¢ncia das features nos modelos confirma os achados da anÃ¡lise exploratÃ³ria:
    
    - Despertares sÃ£o o fator mais prejudicial
    - Ãlcool tem forte impacto negativo
    - ExercÃ­cios sÃ£o o principal aliado

## Principais ConclusÃµes

### Sobre os Dados

1. **PadrÃµes NÃ£o-Lineares Dominam**
   - Modelos lineares tÃªm performance limitada (~65% RÂ²)
   - Modelos tree-based capturam melhor as interaÃ§Ãµes complexas

2. **Fatores Mais Impactantes**
   - **Despertares noturnos**: CorrelaÃ§Ã£o -0.55
   - **Consumo de Ã¡lcool**: CorrelaÃ§Ã£o -0.38
   - **FrequÃªncia de exercÃ­cios**: CorrelaÃ§Ã£o +0.26

3. **DuraÃ§Ã£o â‰  Qualidade**
   - DuraÃ§Ã£o do sono tem correlaÃ§Ã£o quase nula com eficiÃªncia (+0.08)
   - Foco deve estar na **qualidade**, nÃ£o apenas na quantidade

### Sobre a Modelagem

1. **Ensemble Methods Superiores**
   - Gradient Boosting e Random Forest superam significativamente outros modelos
   - DiferenÃ§a de ~15-20 pontos percentuais em RÂ² vs modelos simples

2. **GeneralizaÃ§Ã£o Robusta**
   - K-Fold com 30 folds garante estimativas confiÃ¡veis
   - Baixa variÃ¢ncia nos modelos ensemble indica estabilidade

3. **Tuning Ã‰ Importante**
   - GridSearchCV melhora performance em 2-5%
   - Especialmente crÃ­tico para SVR e Ã¡rvores individuais

## RecomendaÃ§Ãµes PrÃ¡ticas

### Para Melhoria da EficiÃªncia do Sono

Com base nos achados do modelo:

!!! tip "HÃ¡bitos Recomendados"
    1. **Pratique exercÃ­cios regularmente** (3-5x por semana)
       - Impacto positivo consistente na eficiÃªncia do sono
    
    2. **Evite Ã¡lcool antes de dormir**
       - Reduz significativamente sono REM e profundo
    
    3. **Minimize despertares noturnos**
       - Crie ambiente propÃ­cio: escuro, silencioso, temperatura adequada
    
    4. **Limite cafeÃ­na**
       - Especialmente nas 6-8 horas antes de dormir

!!! warning "NÃ£o Foque Apenas na DuraÃ§Ã£o"
    - Dormir 9 horas com baixa eficiÃªncia Ã© pior que 7 horas com alta eficiÃªncia
    - Qualidade > Quantidade

### Para Trabalhos Futuros

1. **Coletar mais dados**
   - Dataset maior permitiria modelos mais complexos
   - ValidaÃ§Ã£o em dados externos fortaleceria as conclusÃµes

2. **Features adicionais**
   - Temperatura do ambiente
   - RuÃ­do noturno
   - Uso de eletrÃ´nicos antes de dormir
   - MedicaÃ§Ãµes

3. **Modelos mais sofisticados**
   - XGBoost, LightGBM (variantes de Gradient Boosting)
   - Redes Neurais (se mais dados estiverem disponÃ­veis)
   - Stacking de modelos

4. **AnÃ¡lise temporal**
   - Estudar evoluÃ§Ã£o do sono ao longo do tempo
   - Identificar padrÃµes semanais/sazonais

## Resumo Executivo

### Modelo Final Recomendado

**Algoritmo:** Gradient Boosting (ou Random Forest como alternativa robusta)

**Performance:**
- RÂ² â‰ˆ 0.85-0.90 (explica 85-90% da variÃ¢ncia)
- MAE â‰ˆ 0.045-0.050 (erro mÃ©dio de ~5 pontos percentuais)
- EstÃ¡vel e confiÃ¡vel para uso prÃ¡tico

### Fatores CrÃ­ticos para EficiÃªncia do Sono

| Fator | Impacto | RecomendaÃ§Ã£o |
|-------|---------|--------------|
| ğŸ›ï¸ Despertares | Muito Negativo (-0.55) | Criar ambiente ideal para sono contÃ­nuo |
| ğŸ· Ãlcool | Negativo (-0.38) | Evitar, especialmente prÃ³ximo ao horÃ¡rio de dormir |
| ğŸƒ ExercÃ­cios | Positivo (+0.26) | Praticar regularmente (3-5x/semana) |
| â˜• CafeÃ­na | Leve Negativo (-0.15) | Limitar nas horas que antecedem o sono |

### LimitaÃ§Ãµes

!!! warning "ConsideraÃ§Ãµes Importantes"
    - **Tamanho do dataset:** 452 amostras Ã© relativamente pequeno
    - **Causalidade:** CorrelaÃ§Ãµes nÃ£o implicam causalidade
    - **Outliers:** Alguns casos extremos podem influenciar resultados
    - **GeneralizaÃ§Ã£o:** Resultados aplicam-se Ã  populaÃ§Ã£o estudada

## ğŸ”— DocumentaÃ§Ã£o Completa

Para explorar mais detalhes:

- [Dataset](dataset.md) - DescriÃ§Ã£o completa das variÃ¡veis
- [EDA](eda.md) - AnÃ¡lise exploratÃ³ria detalhada
- [PrÃ©-processamento](preprocessing.md) - Pipeline de transformaÃ§Ãµes
- [Modelagem](modeling.md) - Detalhes dos algoritmos
- [Como Usar](setup.md) - Reproduzir o projeto

---

**Equipe:**
- Alan de Oliveira GonÃ§alves
- Ayrton Lucas Viana Albuquerque Silva
- Cauan Halison Arantes de Oliveira
- Hosana Maria Ferro Dias
