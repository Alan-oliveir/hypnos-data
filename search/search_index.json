{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"An\u00e1lise da Efici\u00eancia do Sono Objetivo do Projeto O objetivo deste projeto \u00e9 analisar e identificar padr\u00f5es e correla\u00e7\u00f5es entre a efici\u00eancia do sono e diversos fatores de estilo de vida. Tais informa\u00e7\u00f5es podem oferecer insights valiosos sobre os principais aspectos que influenciam a qualidade do sono. Justificativa A qualidade do sono \u00e9 essencial para a sa\u00fade e o bem-estar f\u00edsico, mental e emocional. Dormir mal pode afetar a produtividade e aumentar o risco de doen\u00e7as cardiovasculares, doen\u00e7as metab\u00f3licas e dist\u00farbios mentais. A an\u00e1lise da efici\u00eancia do sono, considerando fatores de estilo de vida, como rotinas e h\u00e1bitos de consumo, pode contribuir para o desenvolvimento de estrat\u00e9gias que influenciem na melhoria da qualidade do sono e a sa\u00fade em geral. Resumo do Projeto Este projeto de Data Science foi desenvolvido como trabalho final do curso e est\u00e1 estruturado em tr\u00eas etapas principais: 1. An\u00e1lise Explorat\u00f3ria de Dados (EDA) An\u00e1lise descritiva das vari\u00e1veis Visualiza\u00e7\u00f5es de distribui\u00e7\u00f5es An\u00e1lise de correla\u00e7\u00f5es Testes de 12 hip\u00f3teses sobre fatores que afetam o sono 2. Pr\u00e9-processamento e Engenharia de Features Tratamento de valores faltantes Transforma\u00e7\u00e3o de vari\u00e1veis temporais em features c\u00edclicas Normaliza\u00e7\u00e3o e encoding de vari\u00e1veis Pipeline automatizado de pr\u00e9-processamento 3. Modelagem Preditiva Compara\u00e7\u00e3o de 7 algoritmos de Machine Learning Valida\u00e7\u00e3o cruzada com K-Fold (30 folds) Otimiza\u00e7\u00e3o de hiperpar\u00e2metros An\u00e1lise de m\u00e9tricas (R\u00b2, MAE, MSE) Principais Descobertas Fatores Positivos Exerc\u00edcios F\u00edsicos : Correla\u00e7\u00e3o positiva (+0.26) com efici\u00eancia do sono Pr\u00e1tica regular melhora significativamente a qualidade do sono Fatores Negativos Despertares Noturnos : Forte correla\u00e7\u00e3o negativa (-0.55) Consumo de \u00c1lcool : Correla\u00e7\u00e3o negativa (-0.38) Ambos prejudicam significativamente a efici\u00eancia do sono Tecnologias Utilizadas Equipe de Desenvolvimento Alan de Oliveira Gon\u00e7alves Ayrton Lucas Viana Albuquerque Silva Cauan Halison Arantes de Oliveira Hosana Maria Ferro Dias Disciplina: Ci\u00eancia de Dados Professor: Madson Dias Navega\u00e7\u00e3o R\u00e1pida Dataset Conhe\u00e7a o conjunto de dados Sleep Efficiency do Kaggle Explorar Dataset An\u00e1lise Explorat\u00f3ria Visualiza\u00e7\u00f5es, estat\u00edsticas e insights dos dados Ver An\u00e1lises Modelagem Compara\u00e7\u00e3o de algoritmos de Machine Learning Ver Modelos Resultados Conclus\u00f5es e principais achados do projeto Ver Resultados","title":"Home"},{"location":"#analise-da-eficiencia-do-sono","text":"","title":"An\u00e1lise da Efici\u00eancia do Sono"},{"location":"#objetivo-do-projeto","text":"O objetivo deste projeto \u00e9 analisar e identificar padr\u00f5es e correla\u00e7\u00f5es entre a efici\u00eancia do sono e diversos fatores de estilo de vida. Tais informa\u00e7\u00f5es podem oferecer insights valiosos sobre os principais aspectos que influenciam a qualidade do sono.","title":"Objetivo do Projeto"},{"location":"#justificativa","text":"A qualidade do sono \u00e9 essencial para a sa\u00fade e o bem-estar f\u00edsico, mental e emocional. Dormir mal pode afetar a produtividade e aumentar o risco de doen\u00e7as cardiovasculares, doen\u00e7as metab\u00f3licas e dist\u00farbios mentais. A an\u00e1lise da efici\u00eancia do sono, considerando fatores de estilo de vida, como rotinas e h\u00e1bitos de consumo, pode contribuir para o desenvolvimento de estrat\u00e9gias que influenciem na melhoria da qualidade do sono e a sa\u00fade em geral.","title":"Justificativa"},{"location":"#resumo-do-projeto","text":"Este projeto de Data Science foi desenvolvido como trabalho final do curso e est\u00e1 estruturado em tr\u00eas etapas principais:","title":"Resumo do Projeto"},{"location":"#1-analise-exploratoria-de-dados-eda","text":"An\u00e1lise descritiva das vari\u00e1veis Visualiza\u00e7\u00f5es de distribui\u00e7\u00f5es An\u00e1lise de correla\u00e7\u00f5es Testes de 12 hip\u00f3teses sobre fatores que afetam o sono","title":"1. An\u00e1lise Explorat\u00f3ria de Dados (EDA)"},{"location":"#2-pre-processamento-e-engenharia-de-features","text":"Tratamento de valores faltantes Transforma\u00e7\u00e3o de vari\u00e1veis temporais em features c\u00edclicas Normaliza\u00e7\u00e3o e encoding de vari\u00e1veis Pipeline automatizado de pr\u00e9-processamento","title":"2. Pr\u00e9-processamento e Engenharia de Features"},{"location":"#3-modelagem-preditiva","text":"Compara\u00e7\u00e3o de 7 algoritmos de Machine Learning Valida\u00e7\u00e3o cruzada com K-Fold (30 folds) Otimiza\u00e7\u00e3o de hiperpar\u00e2metros An\u00e1lise de m\u00e9tricas (R\u00b2, MAE, MSE)","title":"3. Modelagem Preditiva"},{"location":"#principais-descobertas","text":"Fatores Positivos Exerc\u00edcios F\u00edsicos : Correla\u00e7\u00e3o positiva (+0.26) com efici\u00eancia do sono Pr\u00e1tica regular melhora significativamente a qualidade do sono Fatores Negativos Despertares Noturnos : Forte correla\u00e7\u00e3o negativa (-0.55) Consumo de \u00c1lcool : Correla\u00e7\u00e3o negativa (-0.38) Ambos prejudicam significativamente a efici\u00eancia do sono","title":"Principais Descobertas"},{"location":"#tecnologias-utilizadas","text":"","title":"Tecnologias Utilizadas"},{"location":"#equipe-de-desenvolvimento","text":"Alan de Oliveira Gon\u00e7alves Ayrton Lucas Viana Albuquerque Silva Cauan Halison Arantes de Oliveira Hosana Maria Ferro Dias Disciplina: Ci\u00eancia de Dados Professor: Madson Dias","title":"Equipe de Desenvolvimento"},{"location":"#navegacao-rapida","text":"Dataset Conhe\u00e7a o conjunto de dados Sleep Efficiency do Kaggle Explorar Dataset An\u00e1lise Explorat\u00f3ria Visualiza\u00e7\u00f5es, estat\u00edsticas e insights dos dados Ver An\u00e1lises Modelagem Compara\u00e7\u00e3o de algoritmos de Machine Learning Ver Modelos Resultados Conclus\u00f5es e principais achados do projeto Ver Resultados","title":"Navega\u00e7\u00e3o R\u00e1pida"},{"location":"dataset/","text":"Dataset: Sleep Efficiency Fonte dos Dados O conjunto de dados utilizado neste projeto \u00e9 o Sleep Efficiency , dispon\u00edvel publicamente no Kaggle. Fonte: Sleep Efficiency Dataset - Kaggle URL do Dataset: https://raw.githubusercontent.com/atlantico-academy/datasets/refs/heads/main/sleep_efficiency.csv Vis\u00e3o Geral O dataset cont\u00e9m informa\u00e7\u00f5es sobre padr\u00f5es de sono e fatores de estilo de vida de 452 participantes, com 15 vari\u00e1veis que descrevem caracter\u00edsticas demogr\u00e1ficas, comportamentais e m\u00e9tricas relacionadas ao sono. Estat\u00edsticas B\u00e1sicas Total de Amostras: 452 registros Total de Vari\u00e1veis: 15 Per\u00edodo de Coleta: N\u00e3o especificado Valores Faltantes: Presentes em algumas vari\u00e1veis (tratados durante o pr\u00e9-processamento) Dicion\u00e1rio de Dados Vari\u00e1veis Identificadoras Vari\u00e1vel Tipo Descri\u00e7\u00e3o ID Quantitativa Discreta Identifica\u00e7\u00e3o \u00fanica do paciente Vari\u00e1veis Demogr\u00e1ficas Vari\u00e1vel Tipo Descri\u00e7\u00e3o Age Quantitativa Discreta Idade do paciente (anos) Gender Qualitativa Nominal G\u00eanero do paciente (Male/Female) Vari\u00e1veis Temporais Vari\u00e1vel Tipo Descri\u00e7\u00e3o Bedtime Qualitativa Ordinal Hora que cada paciente vai para a cama Wakeup time Qualitativa Ordinal Hora que cada paciente acorda M\u00e9tricas de Sono Vari\u00e1vel Tipo Descri\u00e7\u00e3o Sleep duration Quantitativa Cont\u00ednua Dura\u00e7\u00e3o total do sono em horas Sleep efficiency Quantitativa Cont\u00ednua Propor\u00e7\u00e3o do tempo na cama que foi realmente gasto dormindo (0-1) \u2b50 Vari\u00e1vel Alvo REM sleep percentage Quantitativa Discreta Percentual de tempo gasto em sono REM Deep sleep percentage Quantitativa Discreta Percentual de tempo gasto em sono profundo Light sleep percentage Quantitativa Discreta Percentual de tempo gasto em sono leve Awakenings Quantitativa Discreta N\u00famero de vezes que o indiv\u00edduo acorda durante a noite Fatores de Estilo de Vida Vari\u00e1vel Tipo Descri\u00e7\u00e3o Caffeine consumption Quantitativa Discreta Consumo de cafe\u00edna nas 24 horas anteriores (mg) Alcohol consumption Quantitativa Discreta Consumo de \u00e1lcool nas 24 horas anteriores (doses) Smoking status Qualitativa Nominal Status de fumante (Yes/No) Exercise frequency Qualitativa Ordinal Frequ\u00eancia de exerc\u00edcios por semana (dias) Vari\u00e1vel Alvo A vari\u00e1vel principal de an\u00e1lise e predi\u00e7\u00e3o \u00e9 a Sleep efficiency (Efici\u00eancia do Sono), que representa a propor\u00e7\u00e3o do tempo na cama que foi efetivamente gasto dormindo. F\u00f3rmula: $$ \\text{Sleep Efficiency} = \\frac{\\text{Tempo Real de Sono}}{\\text{Tempo Total na Cama}} $$ Valores pr\u00f3ximos de 1.0 (ou 100%) indicam alta efici\u00eancia do sono. Classifica\u00e7\u00e3o das Vari\u00e1veis Por Tipo Estat\u00edstico Quantitativas Cont\u00ednuas: - Sleep duration - Sleep efficiency - Awakenings Quantitativas Discretas: - ID - Age - REM Sleep percentage - Deep sleep percentage - Light sleep percentage - Caffeine consumption - Alcohol consumption Qualitativas Nominais: - Gender - Smoking status Qualitativas Ordinais: - Bedtime - Wakeup time - Exercise frequency Observa\u00e7\u00f5es Iniciais Caracter\u00edsticas do Dataset Distribui\u00e7\u00e3o de G\u00eanero: Leve predomin\u00e2ncia masculina Idade M\u00e9dia: 40 anos Dura\u00e7\u00e3o M\u00e9dia do Sono: 7 horas Dura\u00e7\u00e3o M\u00ednima: 5 horas Status de Fumante: Maioria n\u00e3o fumante Exerc\u00edcio F\u00edsico: Varia\u00e7\u00e3o ampla, com picos em 0 e 3 dias por semana Valores Faltantes Algumas vari\u00e1veis apresentam valores faltantes que foram tratados durante o pr\u00e9-processamento: Awakenings Caffeine consumption Alcohol consumption Exercise frequency Estrat\u00e9gia: Imputa\u00e7\u00e3o pela mediana (vari\u00e1veis num\u00e9ricas) Pr\u00f3ximos Passos Ap\u00f3s compreender a estrutura do dataset, explore as an\u00e1lises realizadas: An\u00e1lise Explorat\u00f3ria de Dados - Visualiza\u00e7\u00f5es e estat\u00edsticas descritivas Pr\u00e9-processamento - Tratamento de dados e engenharia de features Modelagem - Algoritmos de Machine Learning aplicados","title":"Dataset"},{"location":"dataset/#dataset-sleep-efficiency","text":"","title":"Dataset: Sleep Efficiency"},{"location":"dataset/#fonte-dos-dados","text":"O conjunto de dados utilizado neste projeto \u00e9 o Sleep Efficiency , dispon\u00edvel publicamente no Kaggle. Fonte: Sleep Efficiency Dataset - Kaggle URL do Dataset: https://raw.githubusercontent.com/atlantico-academy/datasets/refs/heads/main/sleep_efficiency.csv","title":"Fonte dos Dados"},{"location":"dataset/#visao-geral","text":"O dataset cont\u00e9m informa\u00e7\u00f5es sobre padr\u00f5es de sono e fatores de estilo de vida de 452 participantes, com 15 vari\u00e1veis que descrevem caracter\u00edsticas demogr\u00e1ficas, comportamentais e m\u00e9tricas relacionadas ao sono.","title":"Vis\u00e3o Geral"},{"location":"dataset/#estatisticas-basicas","text":"Total de Amostras: 452 registros Total de Vari\u00e1veis: 15 Per\u00edodo de Coleta: N\u00e3o especificado Valores Faltantes: Presentes em algumas vari\u00e1veis (tratados durante o pr\u00e9-processamento)","title":"Estat\u00edsticas B\u00e1sicas"},{"location":"dataset/#dicionario-de-dados","text":"","title":"Dicion\u00e1rio de Dados"},{"location":"dataset/#variaveis-identificadoras","text":"Vari\u00e1vel Tipo Descri\u00e7\u00e3o ID Quantitativa Discreta Identifica\u00e7\u00e3o \u00fanica do paciente","title":"Vari\u00e1veis Identificadoras"},{"location":"dataset/#variaveis-demograficas","text":"Vari\u00e1vel Tipo Descri\u00e7\u00e3o Age Quantitativa Discreta Idade do paciente (anos) Gender Qualitativa Nominal G\u00eanero do paciente (Male/Female)","title":"Vari\u00e1veis Demogr\u00e1ficas"},{"location":"dataset/#variaveis-temporais","text":"Vari\u00e1vel Tipo Descri\u00e7\u00e3o Bedtime Qualitativa Ordinal Hora que cada paciente vai para a cama Wakeup time Qualitativa Ordinal Hora que cada paciente acorda","title":"Vari\u00e1veis Temporais"},{"location":"dataset/#metricas-de-sono","text":"Vari\u00e1vel Tipo Descri\u00e7\u00e3o Sleep duration Quantitativa Cont\u00ednua Dura\u00e7\u00e3o total do sono em horas Sleep efficiency Quantitativa Cont\u00ednua Propor\u00e7\u00e3o do tempo na cama que foi realmente gasto dormindo (0-1) \u2b50 Vari\u00e1vel Alvo REM sleep percentage Quantitativa Discreta Percentual de tempo gasto em sono REM Deep sleep percentage Quantitativa Discreta Percentual de tempo gasto em sono profundo Light sleep percentage Quantitativa Discreta Percentual de tempo gasto em sono leve Awakenings Quantitativa Discreta N\u00famero de vezes que o indiv\u00edduo acorda durante a noite","title":"M\u00e9tricas de Sono"},{"location":"dataset/#fatores-de-estilo-de-vida","text":"Vari\u00e1vel Tipo Descri\u00e7\u00e3o Caffeine consumption Quantitativa Discreta Consumo de cafe\u00edna nas 24 horas anteriores (mg) Alcohol consumption Quantitativa Discreta Consumo de \u00e1lcool nas 24 horas anteriores (doses) Smoking status Qualitativa Nominal Status de fumante (Yes/No) Exercise frequency Qualitativa Ordinal Frequ\u00eancia de exerc\u00edcios por semana (dias)","title":"Fatores de Estilo de Vida"},{"location":"dataset/#variavel-alvo","text":"A vari\u00e1vel principal de an\u00e1lise e predi\u00e7\u00e3o \u00e9 a Sleep efficiency (Efici\u00eancia do Sono), que representa a propor\u00e7\u00e3o do tempo na cama que foi efetivamente gasto dormindo. F\u00f3rmula: $$ \\text{Sleep Efficiency} = \\frac{\\text{Tempo Real de Sono}}{\\text{Tempo Total na Cama}} $$ Valores pr\u00f3ximos de 1.0 (ou 100%) indicam alta efici\u00eancia do sono.","title":"Vari\u00e1vel Alvo"},{"location":"dataset/#classificacao-das-variaveis","text":"","title":"Classifica\u00e7\u00e3o das Vari\u00e1veis"},{"location":"dataset/#por-tipo-estatistico","text":"Quantitativas Cont\u00ednuas: - Sleep duration - Sleep efficiency - Awakenings Quantitativas Discretas: - ID - Age - REM Sleep percentage - Deep sleep percentage - Light sleep percentage - Caffeine consumption - Alcohol consumption Qualitativas Nominais: - Gender - Smoking status Qualitativas Ordinais: - Bedtime - Wakeup time - Exercise frequency","title":"Por Tipo Estat\u00edstico"},{"location":"dataset/#observacoes-iniciais","text":"Caracter\u00edsticas do Dataset Distribui\u00e7\u00e3o de G\u00eanero: Leve predomin\u00e2ncia masculina Idade M\u00e9dia: 40 anos Dura\u00e7\u00e3o M\u00e9dia do Sono: 7 horas Dura\u00e7\u00e3o M\u00ednima: 5 horas Status de Fumante: Maioria n\u00e3o fumante Exerc\u00edcio F\u00edsico: Varia\u00e7\u00e3o ampla, com picos em 0 e 3 dias por semana Valores Faltantes Algumas vari\u00e1veis apresentam valores faltantes que foram tratados durante o pr\u00e9-processamento: Awakenings Caffeine consumption Alcohol consumption Exercise frequency Estrat\u00e9gia: Imputa\u00e7\u00e3o pela mediana (vari\u00e1veis num\u00e9ricas)","title":"Observa\u00e7\u00f5es Iniciais"},{"location":"dataset/#proximos-passos","text":"Ap\u00f3s compreender a estrutura do dataset, explore as an\u00e1lises realizadas: An\u00e1lise Explorat\u00f3ria de Dados - Visualiza\u00e7\u00f5es e estat\u00edsticas descritivas Pr\u00e9-processamento - Tratamento de dados e engenharia de features Modelagem - Algoritmos de Machine Learning aplicados","title":"Pr\u00f3ximos Passos"},{"location":"eda/","text":"An\u00e1lise Explorat\u00f3ria de Dados (EDA) Objetivos da An\u00e1lise A An\u00e1lise Explorat\u00f3ria de Dados (EDA) tem como objetivo compreender a estrutura do dataset, identificar padr\u00f5es, detectar anomalias e formular hip\u00f3teses sobre os fatores que influenciam a efici\u00eancia do sono. Estat\u00edsticas Descritivas Vari\u00e1veis Quantitativas As principais m\u00e9tricas estat\u00edsticas das vari\u00e1veis num\u00e9ricas revelam: Idade: Distribui\u00e7\u00e3o ampla, m\u00e9dia de 40 anos Dura\u00e7\u00e3o do Sono: M\u00e9dia de 7 horas, com varia\u00e7\u00e3o de 5 a 9 horas Efici\u00eancia do Sono: Distribui\u00e7\u00e3o concentrada entre 0.7 e 0.9 Despertares: M\u00e9dia de aproximadamente 1-2 por noite Consumo de Cafe\u00edna: Varia\u00e7\u00e3o significativa entre participantes Consumo de \u00c1lcool: Maioria com consumo baixo ou nulo Vari\u00e1veis Qualitativas Distribui\u00e7\u00e3o por G\u00eanero: - Leve predomin\u00e2ncia do g\u00eanero masculino - Distribui\u00e7\u00e3o relativamente equilibrada Status de Fumante: - Maioria n\u00e3o fumante - Pequena propor\u00e7\u00e3o de fumantes ativos Frequ\u00eancia de Exerc\u00edcios: - Distribui\u00e7\u00e3o bimodal: picos em 0 dias/semana e 3 dias/semana - Varia\u00e7\u00e3o ampla entre participantes An\u00e1lise Bivariada Foram testadas 12 hip\u00f3teses para investigar rela\u00e7\u00f5es entre a efici\u00eancia do sono e diversos fatores. Hip\u00f3teses sobre Efici\u00eancia do Sono Hip\u00f3tese 1: Efici\u00eancia do Sono vs. Idade Correla\u00e7\u00e3o de Pearson: Fraca correla\u00e7\u00e3o negativa Interpreta\u00e7\u00e3o: A idade tem impacto limitado na efici\u00eancia do sono. Embora haja uma leve tend\u00eancia de redu\u00e7\u00e3o com o aumento da idade, outros fatores s\u00e3o mais determinantes. Hip\u00f3tese 2: Efici\u00eancia do Sono vs. Status de Fumante Resultado: - N\u00e3o fumantes: Efici\u00eancia ligeiramente superior - Fumantes: Efici\u00eancia ligeiramente inferior Interpreta\u00e7\u00e3o: O tabagismo mostra algum impacto negativo, mas n\u00e3o \u00e9 o fator mais determinante. Hip\u00f3tese 3: Efici\u00eancia do Sono vs. Frequ\u00eancia de Exerc\u00edcios Resultado: Rela\u00e7\u00e3o positiva clara Correla\u00e7\u00e3o: +0.26 (positiva moderada) Interpreta\u00e7\u00e3o: A pr\u00e1tica regular de exerc\u00edcios f\u00edsicos est\u00e1 associada a melhor efici\u00eancia do sono. Este \u00e9 um dos fatores positivos mais importantes identificados. Qualidade do Sono e Exerc\u00edcios F\u00edsicos Hip\u00f3tese 4: Sono Leve vs. Frequ\u00eancia de Exerc\u00edcios Resultado: Pessoas que se exercitam regularmente tendem a ter menor percentual de sono leve. Hip\u00f3tese 5: Sono REM vs. Frequ\u00eancia de Exerc\u00edcios Resultado: Exerc\u00edcios regulares est\u00e3o associados a maiores percentuais de sono REM. Hip\u00f3tese 6: Sono Profundo vs. Frequ\u00eancia de Exerc\u00edcios Resultado: Exerc\u00edcios f\u00edsicos favorecem o aumento do sono profundo, essencial para recupera\u00e7\u00e3o f\u00edsica. Conclus\u00e3o - Exerc\u00edcios A pr\u00e1tica regular de exerc\u00edcios f\u00edsicos melhora n\u00e3o apenas a efici\u00eancia total do sono, mas tamb\u00e9m favorece est\u00e1gios mais reparadores (REM e profundo) em detrimento do sono leve. Qualidade do Sono e Consumo de \u00c1lcool Hip\u00f3tese 7: Sono Leve vs. Consumo de \u00c1lcool Resultado: Maior consumo de \u00e1lcool est\u00e1 associado a maior percentual de sono leve. Hip\u00f3tese 8: Sono REM vs. Consumo de \u00c1lcool Resultado: O \u00e1lcool reduz significativamente o sono REM, prejudicando a qualidade. Hip\u00f3tese 9: Sono Profundo vs. Consumo de \u00c1lcool Resultado: O consumo de \u00e1lcool tamb\u00e9m reduz o sono profundo. Conclus\u00e3o - \u00c1lcool O consumo de \u00e1lcool tem um impacto fortemente negativo na qualidade do sono, reduzindo os est\u00e1gios reparadores (REM e profundo) e aumentando o sono leve. Correla\u00e7\u00e3o: -0.38 Qualidade do Sono e Consumo de Cafe\u00edna Hip\u00f3tese 10: Sono Leve vs. Consumo de Cafe\u00edna Resultado: Consumo elevado de cafe\u00edna est\u00e1 associado a maior percentual de sono leve. Hip\u00f3tese 11: Sono REM vs. Consumo de Cafe\u00edna Resultado: A cafe\u00edna tende a reduzir o sono REM. Hip\u00f3tese 12: Sono Profundo vs. Consumo de Cafe\u00edna Resultado: O consumo de cafe\u00edna nas 24 horas anteriores prejudica o sono profundo. Conclus\u00e3o - Cafe\u00edna Embora com impacto menor que o \u00e1lcool, o consumo de cafe\u00edna (especialmente pr\u00f3ximo ao hor\u00e1rio de dormir) prejudica os est\u00e1gios profundos do sono. An\u00e1lise Multivariada Matriz de Correla\u00e7\u00e3o A an\u00e1lise de correla\u00e7\u00e3o entre todas as vari\u00e1veis num\u00e9ricas revela: Correla\u00e7\u00f5es Mais Fortes com Sleep Efficiency: Vari\u00e1vel Correla\u00e7\u00e3o Interpreta\u00e7\u00e3o Awakenings -0.55 Forte impacto negativo Alcohol consumption -0.38 Impacto negativo moderado Exercise frequency +0.26 Impacto positivo moderado Caffeine consumption -0.15 Impacto negativo leve Age -0.10 Impacto muito fraco Sleep duration +0.08 Praticamente independente Insights da Correla\u00e7\u00e3o Descoberta Importante Dura\u00e7\u00e3o vs. Qualidade: A dura\u00e7\u00e3o do sono tem correla\u00e7\u00e3o quase nula com a efici\u00eancia (+0.08). Isso significa que dormir por mais tempo n\u00e3o garante melhor qualidade de sono . Maior Vil\u00e3o Os despertares noturnos s\u00e3o o fator que mais prejudica a efici\u00eancia do sono (correla\u00e7\u00e3o de -0.55), seguidos pelo consumo de \u00e1lcool (-0.38). Maior Aliado A frequ\u00eancia de exerc\u00edcios \u00e9 o principal fator ben\u00e9fico identificado (+0.26), seguido pela aus\u00eancia de consumo de \u00e1lcool e cafe\u00edna. Principais Conclus\u00f5es da EDA Padr\u00f5es Identificados Exerc\u00edcio F\u00edsico Regular: Melhora significativa na efici\u00eancia e qualidade do sono Consumo de \u00c1lcool: Forte impacto negativo, especialmente em est\u00e1gios REM e profundo Despertares Noturnos: Principal fator prejudicial \u00e0 efici\u00eancia Dura\u00e7\u00e3o \u2260 Qualidade: Tempo de sono n\u00e3o garante efici\u00eancia Tratamento de Dados Valores Faltantes: Identificados e tratados com imputa\u00e7\u00e3o pela mediana Outliers: Detectados, mas mantidos para an\u00e1lise (podem representar casos reais de dist\u00farbios do sono) Convers\u00e3o de Tipos: Datas e hor\u00e1rios convertidos para formato apropriado Implica\u00e7\u00f5es para Modelagem Os insights da EDA guiam as pr\u00f3ximas etapas: Features mais relevantes: Awakenings, Alcohol consumption, Exercise frequency Necessidade de normaliza\u00e7\u00e3o: Vari\u00e1veis em escalas diferentes Encoding de categ\u00f3ricas: Gender, Smoking status Engenharia de features: Transforma\u00e7\u00e3o de hor\u00e1rios em componentes c\u00edclicos Pr\u00f3ximos Passos Com os padr\u00f5es identificados, avan\u00e7amos para: Pr\u00e9-processamento - Prepara\u00e7\u00e3o dos dados para modelagem Modelagem - Aplica\u00e7\u00e3o de algoritmos de Machine Learning Resultados - An\u00e1lise final e conclus\u00f5es","title":"An\u00e1lise Explorat\u00f3ria"},{"location":"eda/#analise-exploratoria-de-dados-eda","text":"","title":"An\u00e1lise Explorat\u00f3ria de Dados (EDA)"},{"location":"eda/#objetivos-da-analise","text":"A An\u00e1lise Explorat\u00f3ria de Dados (EDA) tem como objetivo compreender a estrutura do dataset, identificar padr\u00f5es, detectar anomalias e formular hip\u00f3teses sobre os fatores que influenciam a efici\u00eancia do sono.","title":"Objetivos da An\u00e1lise"},{"location":"eda/#estatisticas-descritivas","text":"","title":"Estat\u00edsticas Descritivas"},{"location":"eda/#variaveis-quantitativas","text":"As principais m\u00e9tricas estat\u00edsticas das vari\u00e1veis num\u00e9ricas revelam: Idade: Distribui\u00e7\u00e3o ampla, m\u00e9dia de 40 anos Dura\u00e7\u00e3o do Sono: M\u00e9dia de 7 horas, com varia\u00e7\u00e3o de 5 a 9 horas Efici\u00eancia do Sono: Distribui\u00e7\u00e3o concentrada entre 0.7 e 0.9 Despertares: M\u00e9dia de aproximadamente 1-2 por noite Consumo de Cafe\u00edna: Varia\u00e7\u00e3o significativa entre participantes Consumo de \u00c1lcool: Maioria com consumo baixo ou nulo","title":"Vari\u00e1veis Quantitativas"},{"location":"eda/#variaveis-qualitativas","text":"Distribui\u00e7\u00e3o por G\u00eanero: - Leve predomin\u00e2ncia do g\u00eanero masculino - Distribui\u00e7\u00e3o relativamente equilibrada Status de Fumante: - Maioria n\u00e3o fumante - Pequena propor\u00e7\u00e3o de fumantes ativos Frequ\u00eancia de Exerc\u00edcios: - Distribui\u00e7\u00e3o bimodal: picos em 0 dias/semana e 3 dias/semana - Varia\u00e7\u00e3o ampla entre participantes","title":"Vari\u00e1veis Qualitativas"},{"location":"eda/#analise-bivariada","text":"Foram testadas 12 hip\u00f3teses para investigar rela\u00e7\u00f5es entre a efici\u00eancia do sono e diversos fatores.","title":"An\u00e1lise Bivariada"},{"location":"eda/#hipoteses-sobre-eficiencia-do-sono","text":"","title":"Hip\u00f3teses sobre Efici\u00eancia do Sono"},{"location":"eda/#hipotese-1-eficiencia-do-sono-vs-idade","text":"Correla\u00e7\u00e3o de Pearson: Fraca correla\u00e7\u00e3o negativa Interpreta\u00e7\u00e3o: A idade tem impacto limitado na efici\u00eancia do sono. Embora haja uma leve tend\u00eancia de redu\u00e7\u00e3o com o aumento da idade, outros fatores s\u00e3o mais determinantes.","title":"Hip\u00f3tese 1: Efici\u00eancia do Sono vs. Idade"},{"location":"eda/#hipotese-2-eficiencia-do-sono-vs-status-de-fumante","text":"Resultado: - N\u00e3o fumantes: Efici\u00eancia ligeiramente superior - Fumantes: Efici\u00eancia ligeiramente inferior Interpreta\u00e7\u00e3o: O tabagismo mostra algum impacto negativo, mas n\u00e3o \u00e9 o fator mais determinante.","title":"Hip\u00f3tese 2: Efici\u00eancia do Sono vs. Status de Fumante"},{"location":"eda/#hipotese-3-eficiencia-do-sono-vs-frequencia-de-exercicios","text":"Resultado: Rela\u00e7\u00e3o positiva clara Correla\u00e7\u00e3o: +0.26 (positiva moderada) Interpreta\u00e7\u00e3o: A pr\u00e1tica regular de exerc\u00edcios f\u00edsicos est\u00e1 associada a melhor efici\u00eancia do sono. Este \u00e9 um dos fatores positivos mais importantes identificados.","title":"Hip\u00f3tese 3: Efici\u00eancia do Sono vs. Frequ\u00eancia de Exerc\u00edcios"},{"location":"eda/#qualidade-do-sono-e-exercicios-fisicos","text":"","title":"Qualidade do Sono e Exerc\u00edcios F\u00edsicos"},{"location":"eda/#hipotese-4-sono-leve-vs-frequencia-de-exercicios","text":"Resultado: Pessoas que se exercitam regularmente tendem a ter menor percentual de sono leve.","title":"Hip\u00f3tese 4: Sono Leve vs. Frequ\u00eancia de Exerc\u00edcios"},{"location":"eda/#hipotese-5-sono-rem-vs-frequencia-de-exercicios","text":"Resultado: Exerc\u00edcios regulares est\u00e3o associados a maiores percentuais de sono REM.","title":"Hip\u00f3tese 5: Sono REM vs. Frequ\u00eancia de Exerc\u00edcios"},{"location":"eda/#hipotese-6-sono-profundo-vs-frequencia-de-exercicios","text":"Resultado: Exerc\u00edcios f\u00edsicos favorecem o aumento do sono profundo, essencial para recupera\u00e7\u00e3o f\u00edsica. Conclus\u00e3o - Exerc\u00edcios A pr\u00e1tica regular de exerc\u00edcios f\u00edsicos melhora n\u00e3o apenas a efici\u00eancia total do sono, mas tamb\u00e9m favorece est\u00e1gios mais reparadores (REM e profundo) em detrimento do sono leve.","title":"Hip\u00f3tese 6: Sono Profundo vs. Frequ\u00eancia de Exerc\u00edcios"},{"location":"eda/#qualidade-do-sono-e-consumo-de-alcool","text":"","title":"Qualidade do Sono e Consumo de \u00c1lcool"},{"location":"eda/#hipotese-7-sono-leve-vs-consumo-de-alcool","text":"Resultado: Maior consumo de \u00e1lcool est\u00e1 associado a maior percentual de sono leve.","title":"Hip\u00f3tese 7: Sono Leve vs. Consumo de \u00c1lcool"},{"location":"eda/#hipotese-8-sono-rem-vs-consumo-de-alcool","text":"Resultado: O \u00e1lcool reduz significativamente o sono REM, prejudicando a qualidade.","title":"Hip\u00f3tese 8: Sono REM vs. Consumo de \u00c1lcool"},{"location":"eda/#hipotese-9-sono-profundo-vs-consumo-de-alcool","text":"Resultado: O consumo de \u00e1lcool tamb\u00e9m reduz o sono profundo. Conclus\u00e3o - \u00c1lcool O consumo de \u00e1lcool tem um impacto fortemente negativo na qualidade do sono, reduzindo os est\u00e1gios reparadores (REM e profundo) e aumentando o sono leve. Correla\u00e7\u00e3o: -0.38","title":"Hip\u00f3tese 9: Sono Profundo vs. Consumo de \u00c1lcool"},{"location":"eda/#qualidade-do-sono-e-consumo-de-cafeina","text":"","title":"Qualidade do Sono e Consumo de Cafe\u00edna"},{"location":"eda/#hipotese-10-sono-leve-vs-consumo-de-cafeina","text":"Resultado: Consumo elevado de cafe\u00edna est\u00e1 associado a maior percentual de sono leve.","title":"Hip\u00f3tese 10: Sono Leve vs. Consumo de Cafe\u00edna"},{"location":"eda/#hipotese-11-sono-rem-vs-consumo-de-cafeina","text":"Resultado: A cafe\u00edna tende a reduzir o sono REM.","title":"Hip\u00f3tese 11: Sono REM vs. Consumo de Cafe\u00edna"},{"location":"eda/#hipotese-12-sono-profundo-vs-consumo-de-cafeina","text":"Resultado: O consumo de cafe\u00edna nas 24 horas anteriores prejudica o sono profundo. Conclus\u00e3o - Cafe\u00edna Embora com impacto menor que o \u00e1lcool, o consumo de cafe\u00edna (especialmente pr\u00f3ximo ao hor\u00e1rio de dormir) prejudica os est\u00e1gios profundos do sono.","title":"Hip\u00f3tese 12: Sono Profundo vs. Consumo de Cafe\u00edna"},{"location":"eda/#analise-multivariada","text":"","title":"An\u00e1lise Multivariada"},{"location":"eda/#matriz-de-correlacao","text":"A an\u00e1lise de correla\u00e7\u00e3o entre todas as vari\u00e1veis num\u00e9ricas revela: Correla\u00e7\u00f5es Mais Fortes com Sleep Efficiency: Vari\u00e1vel Correla\u00e7\u00e3o Interpreta\u00e7\u00e3o Awakenings -0.55 Forte impacto negativo Alcohol consumption -0.38 Impacto negativo moderado Exercise frequency +0.26 Impacto positivo moderado Caffeine consumption -0.15 Impacto negativo leve Age -0.10 Impacto muito fraco Sleep duration +0.08 Praticamente independente","title":"Matriz de Correla\u00e7\u00e3o"},{"location":"eda/#insights-da-correlacao","text":"Descoberta Importante Dura\u00e7\u00e3o vs. Qualidade: A dura\u00e7\u00e3o do sono tem correla\u00e7\u00e3o quase nula com a efici\u00eancia (+0.08). Isso significa que dormir por mais tempo n\u00e3o garante melhor qualidade de sono . Maior Vil\u00e3o Os despertares noturnos s\u00e3o o fator que mais prejudica a efici\u00eancia do sono (correla\u00e7\u00e3o de -0.55), seguidos pelo consumo de \u00e1lcool (-0.38). Maior Aliado A frequ\u00eancia de exerc\u00edcios \u00e9 o principal fator ben\u00e9fico identificado (+0.26), seguido pela aus\u00eancia de consumo de \u00e1lcool e cafe\u00edna.","title":"Insights da Correla\u00e7\u00e3o"},{"location":"eda/#principais-conclusoes-da-eda","text":"","title":"Principais Conclus\u00f5es da EDA"},{"location":"eda/#padroes-identificados","text":"Exerc\u00edcio F\u00edsico Regular: Melhora significativa na efici\u00eancia e qualidade do sono Consumo de \u00c1lcool: Forte impacto negativo, especialmente em est\u00e1gios REM e profundo Despertares Noturnos: Principal fator prejudicial \u00e0 efici\u00eancia Dura\u00e7\u00e3o \u2260 Qualidade: Tempo de sono n\u00e3o garante efici\u00eancia","title":"Padr\u00f5es Identificados"},{"location":"eda/#tratamento-de-dados","text":"Valores Faltantes: Identificados e tratados com imputa\u00e7\u00e3o pela mediana Outliers: Detectados, mas mantidos para an\u00e1lise (podem representar casos reais de dist\u00farbios do sono) Convers\u00e3o de Tipos: Datas e hor\u00e1rios convertidos para formato apropriado","title":"Tratamento de Dados"},{"location":"eda/#implicacoes-para-modelagem","text":"Os insights da EDA guiam as pr\u00f3ximas etapas: Features mais relevantes: Awakenings, Alcohol consumption, Exercise frequency Necessidade de normaliza\u00e7\u00e3o: Vari\u00e1veis em escalas diferentes Encoding de categ\u00f3ricas: Gender, Smoking status Engenharia de features: Transforma\u00e7\u00e3o de hor\u00e1rios em componentes c\u00edclicos","title":"Implica\u00e7\u00f5es para Modelagem"},{"location":"eda/#proximos-passos","text":"Com os padr\u00f5es identificados, avan\u00e7amos para: Pr\u00e9-processamento - Prepara\u00e7\u00e3o dos dados para modelagem Modelagem - Aplica\u00e7\u00e3o de algoritmos de Machine Learning Resultados - An\u00e1lise final e conclus\u00f5es","title":"Pr\u00f3ximos Passos"},{"location":"modeling/","text":"Modelagem e Compara\u00e7\u00e3o de Algoritmos Objetivo Comparar diferentes algoritmos de Machine Learning para predi\u00e7\u00e3o da efici\u00eancia do sono, identificando qual modelo apresenta o melhor desempenho e maior capacidade de generaliza\u00e7\u00e3o. M\u00e9todo de Valida\u00e7\u00e3o K-Fold Cross-Validation Utilizamos K-Fold Cross-Validation com 30 folds para avaliar os modelos de forma robusta. Como Funciona O dataset \u00e9 dividido em 30 subconjuntos (folds) de tamanhos semelhantes Em cada itera\u00e7\u00e3o: 29 folds s\u00e3o usados para treino 1 fold \u00e9 usado para teste O processo se repete 30 vezes, cada fold sendo usado uma vez como teste As m\u00e9tricas finais s\u00e3o a m\u00e9dia das 30 itera\u00e7\u00f5es kf = KFold ( n_splits = 30 , shuffle = True , random_state = 42 ) Por Que K-Fold? Vantagens Uso eficiente dos dados: Todo o dataset \u00e9 usado para treino e teste Estimativa robusta: 30 itera\u00e7\u00f5es reduzem a vari\u00e2ncia da avalia\u00e7\u00e3o Evita vi\u00e9s: N\u00e3o depende de uma \u00fanica divis\u00e3o train/test Ideal para datasets pequenos: Aproveita ao m\u00e1ximo os 452 registros Alternativa: Holdout O m\u00e9todo holdout (divis\u00e3o \u00fanica 80/20) desperdi\u00e7aria dados e produziria estimativas menos confi\u00e1veis para um dataset deste tamanho. Modelos Comparados 1. Dummy Regressor (Baseline) Tipo: Modelo de refer\u00eancia Descri\u00e7\u00e3o: N\u00e3o realiza aprendizado real. Prediz sempre a m\u00e9dia do conjunto de treino. Objetivo: Estabelecer um baseline m\u00ednimo. Qualquer modelo \u00fatil deve superar o Dummy. DummyRegressor ( strategy = 'mean' ) Quando usar: Como refer\u00eancia para validar se modelos mais complexos agregam valor. 2. Linear Regression Tipo: Modelo linear param\u00e9trico Descri\u00e7\u00e3o: Estabelece uma rela\u00e7\u00e3o linear entre features e target, minimizando o erro quadr\u00e1tico m\u00e9dio. Equa\u00e7\u00e3o: $$ \\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n $$ Vantagens: - Simples e interpret\u00e1vel - Treinamento r\u00e1pido - Bom quando h\u00e1 rela\u00e7\u00e3o linear Desvantagens: - Assume linearidade - Sens\u00edvel a outliers - N\u00e3o captura intera\u00e7\u00f5es complexas LinearRegression () 3. Decision Tree Regressor Tipo: Modelo baseado em \u00e1rvore Descri\u00e7\u00e3o: Constr\u00f3i uma estrutura hier\u00e1rquica de decis\u00f5es, dividindo o espa\u00e7o de features em regi\u00f5es homog\u00eaneas. Como funciona: 1. Escolhe a feature e o ponto de corte que melhor separam os dados 2. Divide recursivamente at\u00e9 um crit\u00e9rio de parada 3. Cada folha cont\u00e9m a predi\u00e7\u00e3o (m\u00e9dia dos exemplos naquela regi\u00e3o) Vantagens: - Captura rela\u00e7\u00f5es n\u00e3o lineares - Captura intera\u00e7\u00f5es entre features automaticamente - Interpret\u00e1vel (pode-se visualizar a \u00e1rvore) Desvantagens: - Propenso a overfitting - Inst\u00e1vel (pequenas mudan\u00e7as nos dados alteram a \u00e1rvore) DecisionTreeRegressor ( random_state = 42 ) 4. K-Nearest Neighbors (KNN) Tipo: Modelo baseado em inst\u00e2ncias Descri\u00e7\u00e3o: Para uma nova observa\u00e7\u00e3o, encontra os K exemplos mais pr\u00f3ximos no treino e prediz a m\u00e9dia dos seus valores. Como funciona: 1. Calcula a dist\u00e2ncia (geralmente euclidiana) entre a nova entrada e todos os pontos de treino 2. Seleciona os K vizinhos mais pr\u00f3ximos 3. Prediz a m\u00e9dia dos valores desses vizinhos Vantagens: - N\u00e3o param\u00e9trico (sem suposi\u00e7\u00f5es sobre a forma dos dados) - Simples e intuitivo - Eficaz para padr\u00f5es locais Desvantagens: - Computacionalmente custoso na predi\u00e7\u00e3o - Sens\u00edvel \u00e0 escala das features (requer normaliza\u00e7\u00e3o) - Performance degrada em altas dimens\u00f5es KNeighborsRegressor () 5. Support Vector Regressor (SVR) Tipo: Modelo baseado em margens Descri\u00e7\u00e3o: Encontra uma fun\u00e7\u00e3o que aproxima os dados dentro de uma margem de toler\u00e2ncia (\u03b5), minimizando erro e complexidade. Como funciona: 1. Define uma margem de toler\u00e2ncia ao redor da fun\u00e7\u00e3o de predi\u00e7\u00e3o 2. Ignora erros menores que \u03b5 (support vectors ficam na margem) 3. Penaliza erros maiores que \u03b5 4. Pode usar kernels para capturar n\u00e3o-linearidades Vantagens: - Robusto a outliers - Eficaz em altas dimens\u00f5es - Captura padr\u00f5es complexos com kernels (RBF) Desvantagens: - Treinamento pode ser lento - Escolha do kernel e hiperpar\u00e2metros \u00e9 cr\u00edtica - Menos interpret\u00e1vel SVR () 6. Random Forest Tipo: Ensemble de \u00e1rvores (Bagging) Descri\u00e7\u00e3o: Cria m\u00faltiplas \u00e1rvores de decis\u00e3o independentes e combina suas predi\u00e7\u00f5es atrav\u00e9s da m\u00e9dia. Como funciona: 1. Para cada \u00e1rvore: - Seleciona uma amostra aleat\u00f3ria dos dados (bootstrap) - Em cada divis\u00e3o, considera apenas um subconjunto aleat\u00f3rio de features 2. Treina todas as \u00e1rvores independentemente 3. Predi\u00e7\u00e3o final = m\u00e9dia das predi\u00e7\u00f5es de todas as \u00e1rvores Vantagens: - Reduz overfitting (comparado a uma \u00fanica \u00e1rvore) - Robusto a outliers e ru\u00eddo - Captura intera\u00e7\u00f5es complexas - Menos sens\u00edvel a hiperpar\u00e2metros Desvantagens: - Menos interpret\u00e1vel que uma \u00fanica \u00e1rvore - Pode ser computacionalmente custoso - Mem\u00f3ria necess\u00e1ria para armazenar m\u00faltiplas \u00e1rvores RandomForestRegressor ( random_state = 42 ) 7. Gradient Boosting Tipo: Ensemble de \u00e1rvores (Boosting) Descri\u00e7\u00e3o: Constr\u00f3i \u00e1rvores sequencialmente, onde cada nova \u00e1rvore corrige os erros das anteriores. Como funciona: 1. Come\u00e7a com uma predi\u00e7\u00e3o simples (m\u00e9dia) 2. Para cada itera\u00e7\u00e3o: - Calcula os res\u00edduos (erros) da predi\u00e7\u00e3o atual - Treina uma nova \u00e1rvore para predizer esses res\u00edduos - Adiciona a nova \u00e1rvore ao modelo com um peso (learning rate) 3. Predi\u00e7\u00e3o final = soma ponderada de todas as \u00e1rvores Vantagens: - Geralmente atinge os melhores resultados - Flex\u00edvel (pode otimizar diferentes fun\u00e7\u00f5es de perda) - Captura padr\u00f5es muito complexos Desvantagens: - Mais propenso a overfitting (requer tuning cuidadoso) - Treinamento sequencial (mais lento) - Menos interpret\u00e1vel GradientBoostingRegressor ( random_state = 42 ) M\u00e9tricas de Avalia\u00e7\u00e3o R\u00b2 (Coeficiente de Determina\u00e7\u00e3o) F\u00f3rmula: $$ R^2 = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2} $$ Interpreta\u00e7\u00e3o: - Varia de -\u221e a 1 - R\u00b2 = 1: Predi\u00e7\u00f5es perfeitas - R\u00b2 = 0: Modelo equivalente \u00e0 m\u00e9dia (baseline) - R\u00b2 < 0: Modelo pior que predizer sempre a m\u00e9dia Por qu\u00ea usar: Mede quanto da vari\u00e2ncia do target \u00e9 explicada pelo modelo. Intuitivo e amplamente usado. MAE (Mean Absolute Error) F\u00f3rmula: $$ MAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i| $$ Interpreta\u00e7\u00e3o: - Erro m\u00e9dio absoluto em unidades do target - MAE = 0: Predi\u00e7\u00f5es perfeitas - Quanto menor, melhor Por qu\u00ea usar: F\u00e1cil de interpretar (erro m\u00e9dio na mesma escala do target). Menos sens\u00edvel a outliers que o MSE. MSE (Mean Squared Error) F\u00f3rmula: $$ MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 $$ Interpreta\u00e7\u00e3o: - Erro quadr\u00e1tico m\u00e9dio - Penaliza mais erros grandes (devido ao quadrado) - MSE = 0: Predi\u00e7\u00f5es perfeitas Por qu\u00ea usar: Penaliza mais erros maiores, \u00fatil quando grandes desvios s\u00e3o inaceit\u00e1veis. Implementa\u00e7\u00e3o da Valida\u00e7\u00e3o # Definir m\u00e9tricas scoring_metrics = { 'r2' : 'r2' , 'mae' : 'neg_mean_absolute_error' , 'mse' : 'neg_mean_squared_error' } # Para cada modelo for name , model in models . items (): # Criar pipeline completo pipeline = Pipeline ( steps = [ ( 'preprocessor' , preprocessor ), ( 'model' , model ) ]) # Valida\u00e7\u00e3o cruzada scores = cross_validate ( pipeline , X , y , cv = kf , scoring = scoring_metrics ) # Armazenar resultados all_results [ name ] = scores Importante: O scikit-learn retorna MAE e MSE como valores negativos (para maximiza\u00e7\u00e3o). Por isso invertemos o sinal ao reportar os resultados. Otimiza\u00e7\u00e3o de Hiperpar\u00e2metros Para o melhor modelo identificado (exceto Dummy e Linear Regression), aplicamos GridSearchCV para otimizar hiperpar\u00e2metros. Grids de Busca Decision Tree { 'model__max_depth' : [ 10 , 20 , None ], 'model__min_samples_leaf' : [ 1 , 2 , 4 ], 'model__max_features' : [ 'sqrt' , 'log2' , None ] } KNN { 'model__n_neighbors' : [ 3 , 5 , 7 , 9 ], 'model__weights' : [ 'uniform' , 'distance' ], 'model__p' : [ 1 , 2 ] # Manhattan vs Euclidean } SVR { 'model__C' : [ 0.1 , 1 , 10 ], 'model__kernel' : [ 'linear' , 'rbf' ], 'model__epsilon' : [ 0.01 , 0.1 ] } Random Forest { 'model__n_estimators' : [ 100 , 200 ], 'model__max_depth' : [ 10 , 20 , None ], 'model__min_samples_leaf' : [ 1 , 2 ] } Gradient Boosting { 'model__n_estimators' : [ 100 , 200 ], 'model__learning_rate' : [ 0.01 , 0.1 ], 'model__max_depth' : [ 3 , 5 ] } Execu\u00e7\u00e3o do GridSearchCV grid_search = GridSearchCV ( estimator = pipeline , param_grid = grid_to_use , cv = 10 , # 10 folds (mais r\u00e1pido que 30) scoring = 'r2' , n_jobs =- 1 , # Usa todos os processadores verbose = 1 ) grid_search . fit ( X , y ) print ( f \"Melhor R\u00b2: { grid_search . best_score_ : .4f } \" ) print ( f \"Melhores par\u00e2metros: { grid_search . best_params_ } \" ) An\u00e1lise de Estabilidade Al\u00e9m das m\u00e9dias, analisamos a distribui\u00e7\u00e3o dos scores atrav\u00e9s de boxplots das 30 itera\u00e7\u00f5es. O que buscamos: - Mediana alta: Performance central boa - IQR pequeno: Modelo est\u00e1vel (pouca varia\u00e7\u00e3o entre folds) - Poucos outliers: Comportamento consistente Modelos inst\u00e1veis (grande vari\u00e2ncia) podem ter boa m\u00e9dia mas serem imprevis\u00edveis em produ\u00e7\u00e3o. Pr\u00f3ximos Passos Com os modelos treinados e validados, analisamos: Resultados - Performance comparativa e conclus\u00f5es finais","title":"Modelagem"},{"location":"modeling/#modelagem-e-comparacao-de-algoritmos","text":"","title":"Modelagem e Compara\u00e7\u00e3o de Algoritmos"},{"location":"modeling/#objetivo","text":"Comparar diferentes algoritmos de Machine Learning para predi\u00e7\u00e3o da efici\u00eancia do sono, identificando qual modelo apresenta o melhor desempenho e maior capacidade de generaliza\u00e7\u00e3o.","title":"Objetivo"},{"location":"modeling/#metodo-de-validacao","text":"","title":"M\u00e9todo de Valida\u00e7\u00e3o"},{"location":"modeling/#k-fold-cross-validation","text":"Utilizamos K-Fold Cross-Validation com 30 folds para avaliar os modelos de forma robusta.","title":"K-Fold Cross-Validation"},{"location":"modeling/#como-funciona","text":"O dataset \u00e9 dividido em 30 subconjuntos (folds) de tamanhos semelhantes Em cada itera\u00e7\u00e3o: 29 folds s\u00e3o usados para treino 1 fold \u00e9 usado para teste O processo se repete 30 vezes, cada fold sendo usado uma vez como teste As m\u00e9tricas finais s\u00e3o a m\u00e9dia das 30 itera\u00e7\u00f5es kf = KFold ( n_splits = 30 , shuffle = True , random_state = 42 )","title":"Como Funciona"},{"location":"modeling/#por-que-k-fold","text":"Vantagens Uso eficiente dos dados: Todo o dataset \u00e9 usado para treino e teste Estimativa robusta: 30 itera\u00e7\u00f5es reduzem a vari\u00e2ncia da avalia\u00e7\u00e3o Evita vi\u00e9s: N\u00e3o depende de uma \u00fanica divis\u00e3o train/test Ideal para datasets pequenos: Aproveita ao m\u00e1ximo os 452 registros Alternativa: Holdout O m\u00e9todo holdout (divis\u00e3o \u00fanica 80/20) desperdi\u00e7aria dados e produziria estimativas menos confi\u00e1veis para um dataset deste tamanho.","title":"Por Que K-Fold?"},{"location":"modeling/#modelos-comparados","text":"","title":"Modelos Comparados"},{"location":"modeling/#1-dummy-regressor-baseline","text":"Tipo: Modelo de refer\u00eancia Descri\u00e7\u00e3o: N\u00e3o realiza aprendizado real. Prediz sempre a m\u00e9dia do conjunto de treino. Objetivo: Estabelecer um baseline m\u00ednimo. Qualquer modelo \u00fatil deve superar o Dummy. DummyRegressor ( strategy = 'mean' ) Quando usar: Como refer\u00eancia para validar se modelos mais complexos agregam valor.","title":"1. Dummy Regressor (Baseline)"},{"location":"modeling/#2-linear-regression","text":"Tipo: Modelo linear param\u00e9trico Descri\u00e7\u00e3o: Estabelece uma rela\u00e7\u00e3o linear entre features e target, minimizando o erro quadr\u00e1tico m\u00e9dio. Equa\u00e7\u00e3o: $$ \\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n $$ Vantagens: - Simples e interpret\u00e1vel - Treinamento r\u00e1pido - Bom quando h\u00e1 rela\u00e7\u00e3o linear Desvantagens: - Assume linearidade - Sens\u00edvel a outliers - N\u00e3o captura intera\u00e7\u00f5es complexas LinearRegression ()","title":"2. Linear Regression"},{"location":"modeling/#3-decision-tree-regressor","text":"Tipo: Modelo baseado em \u00e1rvore Descri\u00e7\u00e3o: Constr\u00f3i uma estrutura hier\u00e1rquica de decis\u00f5es, dividindo o espa\u00e7o de features em regi\u00f5es homog\u00eaneas. Como funciona: 1. Escolhe a feature e o ponto de corte que melhor separam os dados 2. Divide recursivamente at\u00e9 um crit\u00e9rio de parada 3. Cada folha cont\u00e9m a predi\u00e7\u00e3o (m\u00e9dia dos exemplos naquela regi\u00e3o) Vantagens: - Captura rela\u00e7\u00f5es n\u00e3o lineares - Captura intera\u00e7\u00f5es entre features automaticamente - Interpret\u00e1vel (pode-se visualizar a \u00e1rvore) Desvantagens: - Propenso a overfitting - Inst\u00e1vel (pequenas mudan\u00e7as nos dados alteram a \u00e1rvore) DecisionTreeRegressor ( random_state = 42 )","title":"3. Decision Tree Regressor"},{"location":"modeling/#4-k-nearest-neighbors-knn","text":"Tipo: Modelo baseado em inst\u00e2ncias Descri\u00e7\u00e3o: Para uma nova observa\u00e7\u00e3o, encontra os K exemplos mais pr\u00f3ximos no treino e prediz a m\u00e9dia dos seus valores. Como funciona: 1. Calcula a dist\u00e2ncia (geralmente euclidiana) entre a nova entrada e todos os pontos de treino 2. Seleciona os K vizinhos mais pr\u00f3ximos 3. Prediz a m\u00e9dia dos valores desses vizinhos Vantagens: - N\u00e3o param\u00e9trico (sem suposi\u00e7\u00f5es sobre a forma dos dados) - Simples e intuitivo - Eficaz para padr\u00f5es locais Desvantagens: - Computacionalmente custoso na predi\u00e7\u00e3o - Sens\u00edvel \u00e0 escala das features (requer normaliza\u00e7\u00e3o) - Performance degrada em altas dimens\u00f5es KNeighborsRegressor ()","title":"4. K-Nearest Neighbors (KNN)"},{"location":"modeling/#5-support-vector-regressor-svr","text":"Tipo: Modelo baseado em margens Descri\u00e7\u00e3o: Encontra uma fun\u00e7\u00e3o que aproxima os dados dentro de uma margem de toler\u00e2ncia (\u03b5), minimizando erro e complexidade. Como funciona: 1. Define uma margem de toler\u00e2ncia ao redor da fun\u00e7\u00e3o de predi\u00e7\u00e3o 2. Ignora erros menores que \u03b5 (support vectors ficam na margem) 3. Penaliza erros maiores que \u03b5 4. Pode usar kernels para capturar n\u00e3o-linearidades Vantagens: - Robusto a outliers - Eficaz em altas dimens\u00f5es - Captura padr\u00f5es complexos com kernels (RBF) Desvantagens: - Treinamento pode ser lento - Escolha do kernel e hiperpar\u00e2metros \u00e9 cr\u00edtica - Menos interpret\u00e1vel SVR ()","title":"5. Support Vector Regressor (SVR)"},{"location":"modeling/#6-random-forest","text":"Tipo: Ensemble de \u00e1rvores (Bagging) Descri\u00e7\u00e3o: Cria m\u00faltiplas \u00e1rvores de decis\u00e3o independentes e combina suas predi\u00e7\u00f5es atrav\u00e9s da m\u00e9dia. Como funciona: 1. Para cada \u00e1rvore: - Seleciona uma amostra aleat\u00f3ria dos dados (bootstrap) - Em cada divis\u00e3o, considera apenas um subconjunto aleat\u00f3rio de features 2. Treina todas as \u00e1rvores independentemente 3. Predi\u00e7\u00e3o final = m\u00e9dia das predi\u00e7\u00f5es de todas as \u00e1rvores Vantagens: - Reduz overfitting (comparado a uma \u00fanica \u00e1rvore) - Robusto a outliers e ru\u00eddo - Captura intera\u00e7\u00f5es complexas - Menos sens\u00edvel a hiperpar\u00e2metros Desvantagens: - Menos interpret\u00e1vel que uma \u00fanica \u00e1rvore - Pode ser computacionalmente custoso - Mem\u00f3ria necess\u00e1ria para armazenar m\u00faltiplas \u00e1rvores RandomForestRegressor ( random_state = 42 )","title":"6. Random Forest"},{"location":"modeling/#7-gradient-boosting","text":"Tipo: Ensemble de \u00e1rvores (Boosting) Descri\u00e7\u00e3o: Constr\u00f3i \u00e1rvores sequencialmente, onde cada nova \u00e1rvore corrige os erros das anteriores. Como funciona: 1. Come\u00e7a com uma predi\u00e7\u00e3o simples (m\u00e9dia) 2. Para cada itera\u00e7\u00e3o: - Calcula os res\u00edduos (erros) da predi\u00e7\u00e3o atual - Treina uma nova \u00e1rvore para predizer esses res\u00edduos - Adiciona a nova \u00e1rvore ao modelo com um peso (learning rate) 3. Predi\u00e7\u00e3o final = soma ponderada de todas as \u00e1rvores Vantagens: - Geralmente atinge os melhores resultados - Flex\u00edvel (pode otimizar diferentes fun\u00e7\u00f5es de perda) - Captura padr\u00f5es muito complexos Desvantagens: - Mais propenso a overfitting (requer tuning cuidadoso) - Treinamento sequencial (mais lento) - Menos interpret\u00e1vel GradientBoostingRegressor ( random_state = 42 )","title":"7. Gradient Boosting"},{"location":"modeling/#metricas-de-avaliacao","text":"","title":"M\u00e9tricas de Avalia\u00e7\u00e3o"},{"location":"modeling/#r2-coeficiente-de-determinacao","text":"F\u00f3rmula: $$ R^2 = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2} $$ Interpreta\u00e7\u00e3o: - Varia de -\u221e a 1 - R\u00b2 = 1: Predi\u00e7\u00f5es perfeitas - R\u00b2 = 0: Modelo equivalente \u00e0 m\u00e9dia (baseline) - R\u00b2 < 0: Modelo pior que predizer sempre a m\u00e9dia Por qu\u00ea usar: Mede quanto da vari\u00e2ncia do target \u00e9 explicada pelo modelo. Intuitivo e amplamente usado.","title":"R\u00b2 (Coeficiente de Determina\u00e7\u00e3o)"},{"location":"modeling/#mae-mean-absolute-error","text":"F\u00f3rmula: $$ MAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i| $$ Interpreta\u00e7\u00e3o: - Erro m\u00e9dio absoluto em unidades do target - MAE = 0: Predi\u00e7\u00f5es perfeitas - Quanto menor, melhor Por qu\u00ea usar: F\u00e1cil de interpretar (erro m\u00e9dio na mesma escala do target). Menos sens\u00edvel a outliers que o MSE.","title":"MAE (Mean Absolute Error)"},{"location":"modeling/#mse-mean-squared-error","text":"F\u00f3rmula: $$ MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 $$ Interpreta\u00e7\u00e3o: - Erro quadr\u00e1tico m\u00e9dio - Penaliza mais erros grandes (devido ao quadrado) - MSE = 0: Predi\u00e7\u00f5es perfeitas Por qu\u00ea usar: Penaliza mais erros maiores, \u00fatil quando grandes desvios s\u00e3o inaceit\u00e1veis.","title":"MSE (Mean Squared Error)"},{"location":"modeling/#implementacao-da-validacao","text":"# Definir m\u00e9tricas scoring_metrics = { 'r2' : 'r2' , 'mae' : 'neg_mean_absolute_error' , 'mse' : 'neg_mean_squared_error' } # Para cada modelo for name , model in models . items (): # Criar pipeline completo pipeline = Pipeline ( steps = [ ( 'preprocessor' , preprocessor ), ( 'model' , model ) ]) # Valida\u00e7\u00e3o cruzada scores = cross_validate ( pipeline , X , y , cv = kf , scoring = scoring_metrics ) # Armazenar resultados all_results [ name ] = scores Importante: O scikit-learn retorna MAE e MSE como valores negativos (para maximiza\u00e7\u00e3o). Por isso invertemos o sinal ao reportar os resultados.","title":"Implementa\u00e7\u00e3o da Valida\u00e7\u00e3o"},{"location":"modeling/#otimizacao-de-hiperparametros","text":"Para o melhor modelo identificado (exceto Dummy e Linear Regression), aplicamos GridSearchCV para otimizar hiperpar\u00e2metros.","title":"Otimiza\u00e7\u00e3o de Hiperpar\u00e2metros"},{"location":"modeling/#grids-de-busca","text":"","title":"Grids de Busca"},{"location":"modeling/#decision-tree","text":"{ 'model__max_depth' : [ 10 , 20 , None ], 'model__min_samples_leaf' : [ 1 , 2 , 4 ], 'model__max_features' : [ 'sqrt' , 'log2' , None ] }","title":"Decision Tree"},{"location":"modeling/#knn","text":"{ 'model__n_neighbors' : [ 3 , 5 , 7 , 9 ], 'model__weights' : [ 'uniform' , 'distance' ], 'model__p' : [ 1 , 2 ] # Manhattan vs Euclidean }","title":"KNN"},{"location":"modeling/#svr","text":"{ 'model__C' : [ 0.1 , 1 , 10 ], 'model__kernel' : [ 'linear' , 'rbf' ], 'model__epsilon' : [ 0.01 , 0.1 ] }","title":"SVR"},{"location":"modeling/#random-forest","text":"{ 'model__n_estimators' : [ 100 , 200 ], 'model__max_depth' : [ 10 , 20 , None ], 'model__min_samples_leaf' : [ 1 , 2 ] }","title":"Random Forest"},{"location":"modeling/#gradient-boosting","text":"{ 'model__n_estimators' : [ 100 , 200 ], 'model__learning_rate' : [ 0.01 , 0.1 ], 'model__max_depth' : [ 3 , 5 ] }","title":"Gradient Boosting"},{"location":"modeling/#execucao-do-gridsearchcv","text":"grid_search = GridSearchCV ( estimator = pipeline , param_grid = grid_to_use , cv = 10 , # 10 folds (mais r\u00e1pido que 30) scoring = 'r2' , n_jobs =- 1 , # Usa todos os processadores verbose = 1 ) grid_search . fit ( X , y ) print ( f \"Melhor R\u00b2: { grid_search . best_score_ : .4f } \" ) print ( f \"Melhores par\u00e2metros: { grid_search . best_params_ } \" )","title":"Execu\u00e7\u00e3o do GridSearchCV"},{"location":"modeling/#analise-de-estabilidade","text":"Al\u00e9m das m\u00e9dias, analisamos a distribui\u00e7\u00e3o dos scores atrav\u00e9s de boxplots das 30 itera\u00e7\u00f5es. O que buscamos: - Mediana alta: Performance central boa - IQR pequeno: Modelo est\u00e1vel (pouca varia\u00e7\u00e3o entre folds) - Poucos outliers: Comportamento consistente Modelos inst\u00e1veis (grande vari\u00e2ncia) podem ter boa m\u00e9dia mas serem imprevis\u00edveis em produ\u00e7\u00e3o.","title":"An\u00e1lise de Estabilidade"},{"location":"modeling/#proximos-passos","text":"Com os modelos treinados e validados, analisamos: Resultados - Performance comparativa e conclus\u00f5es finais","title":"Pr\u00f3ximos Passos"},{"location":"preprocessing/","text":"Pr\u00e9-processamento e Engenharia de Features Objetivos O pr\u00e9-processamento tem como objetivos: Tratar valores faltantes Transformar vari\u00e1veis temporais em features num\u00e9ricas Normalizar vari\u00e1veis num\u00e9ricas Codificar vari\u00e1veis categ\u00f3ricas Criar um pipeline reproduz\u00edvel Limpeza de Dados Remo\u00e7\u00e3o de Colunas A coluna ID foi removida, pois \u00e9 apenas um identificador e n\u00e3o cont\u00e9m informa\u00e7\u00e3o preditiva. df_processed = df_processed . drop ( columns = [ 'ID' ]) Tratamento de Valores Faltantes Foram identificados valores faltantes nas seguintes vari\u00e1veis: Awakenings Caffeine consumption Alcohol consumption Exercise frequency Estrat\u00e9gia Adotada: Imputa\u00e7\u00e3o pela mediana Justificativa: A mediana \u00e9 mais robusta a outliers do que a m\u00e9dia, sendo adequada para vari\u00e1veis que podem ter valores extremos. median_cols = [ 'Awakenings' , 'Caffeine consumption' , 'Alcohol consumption' , 'Exercise frequency' ] for col in median_cols : median_val = df_tratado [ col ] . median () df_tratado [ col ] = df_tratado [ col ] . fillna ( median_val ) Engenharia de Features Transforma\u00e7\u00e3o de Vari\u00e1veis Temporais Os hor\u00e1rios de dormir e acordar foram convertidos para features c\u00edclicas usando fun\u00e7\u00f5es seno e cosseno. Por qu\u00ea? Hor\u00e1rios s\u00e3o c\u00edclicos (23h est\u00e1 pr\u00f3ximo de 0h), e a representa\u00e7\u00e3o tradicional n\u00e3o captura essa ciclicidade. As fun\u00e7\u00f5es trigonom\u00e9tricas resolvem esse problema. Bedtime (Hora de Dormir) df_processed [ 'bedtime_hour_sin' ] = np . sin ( df_processed [ 'Bedtime' ] . dt . hour * ( 2. * np . pi / 24 ) ) df_processed [ 'bedtime_hour_cos' ] = np . cos ( df_processed [ 'Bedtime' ] . dt . hour * ( 2. * np . pi / 24 ) ) Wakeup Time (Hora de Acordar) df_processed [ 'wakeup_hour_sin' ] = np . sin ( df_processed [ 'Wakeup time' ] . dt . hour * ( 2. * np . pi / 24 ) ) df_processed [ 'wakeup_hour_cos' ] = np . cos ( df_processed [ 'Wakeup time' ] . dt . hour * ( 2. * np . pi / 24 ) ) Resultado: 4 novas features c\u00edclicas que capturam melhor a natureza temporal dos hor\u00e1rios. Padroniza\u00e7\u00e3o de Nomes Colunas foram renomeadas para o padr\u00e3o snake_case para facilitar o uso no c\u00f3digo: df_processed . columns = [ col . replace ( ' ' , '_' ) . lower () for col in df_processed . columns ] Exemplos de transforma\u00e7\u00e3o: - Sleep efficiency \u2192 sleep_efficiency - Caffeine consumption \u2192 caffeine_consumption Pipeline de Pr\u00e9-processamento Separa\u00e7\u00e3o de Features e Target TARGET = 'sleep_efficiency' X = df_processed . drop ( columns = [ TARGET ]) y = df_processed [ TARGET ] Identifica\u00e7\u00e3o de Tipos de Vari\u00e1veis numeric_features = X . select_dtypes ( include = np . number ) . columns . tolist () categorical_features = X . select_dtypes ( exclude = np . number ) . columns . tolist () Vari\u00e1veis Num\u00e9ricas: age, sleep_duration, awakenings, caffeine_consumption, alcohol_consumption, exercise_frequency, bedtime_hour_sin, bedtime_hour_cos, wakeup_hour_sin, wakeup_hour_cos, rem_sleep_percentage, deep_sleep_percentage, light_sleep_percentage Vari\u00e1veis Categ\u00f3ricas: gender, smoking_status Pipeline para Vari\u00e1veis Num\u00e9ricas numeric_transformer = Pipeline ( steps = [ ( 'imputer' , SimpleImputer ( strategy = 'median' )), ( 'scaler' , StandardScaler ()) ]) Etapas: SimpleImputer: Preenche valores faltantes com a mediana StandardScaler: Normaliza as vari\u00e1veis (m\u00e9dia 0, desvio padr\u00e3o 1) Por qu\u00ea normalizar? Algoritmos como SVM, KNN e Regress\u00e3o Linear s\u00e3o sens\u00edveis \u00e0 escala das vari\u00e1veis. A normaliza\u00e7\u00e3o garante que todas as features tenham o mesmo peso inicial. Pipeline para Vari\u00e1veis Categ\u00f3ricas categorical_transformer = Pipeline ( steps = [ ( 'imputer' , SimpleImputer ( strategy = 'most_frequent' )), ( 'onehot' , OneHotEncoder ( handle_unknown = 'ignore' , drop = 'first' )) ]) Etapas: SimpleImputer: Preenche valores faltantes com a moda (valor mais frequente) OneHotEncoder: Converte categorias em vari\u00e1veis bin\u00e1rias (0 e 1) handle_unknown='ignore' : Lida com categorias n\u00e3o vistas no treino drop='first' : Remove uma categoria para evitar multicolinearidade Exemplo de Encoding: Gender Original gender_Male Female 0 Male 1 ColumnTransformer O ColumnTransformer aplica os pipelines apropriados para cada tipo de vari\u00e1vel: preprocessor = ColumnTransformer ( transformers = [ ( 'num' , numeric_transformer , numeric_features ), ( 'cat' , categorical_transformer , categorical_features ) ], remainder = 'passthrough' ) Resultado do Pr\u00e9-processamento Ap\u00f3s aplicar o pipeline: X_processed = preprocessor . fit_transform ( X ) print ( f \"Dimens\u00f5es: { X_processed . shape } \" ) # Output: Dimens\u00f5es: (452, 15) O dataset processado est\u00e1 pronto para ser usado nos modelos de Machine Learning. Vantagens do Pipeline Reprodutibilidade O pipeline garante que o mesmo pr\u00e9-processamento ser\u00e1 aplicado de forma consistente: Nos dados de treino Nos dados de valida\u00e7\u00e3o Nos dados de teste Em dados futuros (produ\u00e7\u00e3o) Preven\u00e7\u00e3o de Data Leakage O pipeline garante que: A imputa\u00e7\u00e3o usa estat\u00edsticas calculadas apenas no conjunto de treino A normaliza\u00e7\u00e3o usa m\u00e9dia/desvio padr\u00e3o apenas do treino O OneHotEncoder conhece apenas as categorias do treino Isso evita que informa\u00e7\u00f5es do conjunto de teste \"vazem\" para o treino, o que causaria overfitting e superestima\u00e7\u00e3o da performance. Integra\u00e7\u00e3o com Scikit-Learn O pipeline se integra perfeitamente com: Cross-validation: cross_validate(pipeline, X, y, cv=kfold) GridSearchCV: Permite otimizar hiperpar\u00e2metros do pr\u00e9-processamento e do modelo simultaneamente Modelos finais: O pipeline inteiro pode ser salvo e carregado # Exemplo de uso pipeline = Pipeline ( steps = [ ( 'preprocessor' , preprocessor ), ( 'model' , RandomForestRegressor ()) ]) pipeline . fit ( X_train , y_train ) predictions = pipeline . predict ( X_test ) Resumo das Transforma\u00e7\u00f5es Etapa Entrada Sa\u00edda Justificativa Convers\u00e3o temporal Bedtime, Wakeup time (datetime) 4 features c\u00edclicas (sin, cos) Captura ciclicidade dos hor\u00e1rios Imputa\u00e7\u00e3o num\u00e9rica Features com NaN Features completas (mediana) Mant\u00e9m distribui\u00e7\u00e3o, robusta a outliers Normaliza\u00e7\u00e3o Features em escalas diferentes Features normalizadas (\u03bc=0, \u03c3=1) Equaliza import\u00e2ncia inicial Imputa\u00e7\u00e3o categ\u00f3rica Categorias com NaN Categorias completas (moda) Usa valor mais comum One-Hot Encoding gender, smoking_status Vari\u00e1veis bin\u00e1rias Formato num\u00e9rico para modelos Pr\u00f3ximos Passos Com os dados pr\u00e9-processados, avan\u00e7amos para: Modelagem - Treinamento e compara\u00e7\u00e3o de modelos Resultados - An\u00e1lise de performance e conclus\u00f5es","title":"Pr\u00e9-processamento"},{"location":"preprocessing/#pre-processamento-e-engenharia-de-features","text":"","title":"Pr\u00e9-processamento e Engenharia de Features"},{"location":"preprocessing/#objetivos","text":"O pr\u00e9-processamento tem como objetivos: Tratar valores faltantes Transformar vari\u00e1veis temporais em features num\u00e9ricas Normalizar vari\u00e1veis num\u00e9ricas Codificar vari\u00e1veis categ\u00f3ricas Criar um pipeline reproduz\u00edvel","title":"Objetivos"},{"location":"preprocessing/#limpeza-de-dados","text":"","title":"Limpeza de Dados"},{"location":"preprocessing/#remocao-de-colunas","text":"A coluna ID foi removida, pois \u00e9 apenas um identificador e n\u00e3o cont\u00e9m informa\u00e7\u00e3o preditiva. df_processed = df_processed . drop ( columns = [ 'ID' ])","title":"Remo\u00e7\u00e3o de Colunas"},{"location":"preprocessing/#tratamento-de-valores-faltantes","text":"Foram identificados valores faltantes nas seguintes vari\u00e1veis: Awakenings Caffeine consumption Alcohol consumption Exercise frequency Estrat\u00e9gia Adotada: Imputa\u00e7\u00e3o pela mediana Justificativa: A mediana \u00e9 mais robusta a outliers do que a m\u00e9dia, sendo adequada para vari\u00e1veis que podem ter valores extremos. median_cols = [ 'Awakenings' , 'Caffeine consumption' , 'Alcohol consumption' , 'Exercise frequency' ] for col in median_cols : median_val = df_tratado [ col ] . median () df_tratado [ col ] = df_tratado [ col ] . fillna ( median_val )","title":"Tratamento de Valores Faltantes"},{"location":"preprocessing/#engenharia-de-features","text":"","title":"Engenharia de Features"},{"location":"preprocessing/#transformacao-de-variaveis-temporais","text":"Os hor\u00e1rios de dormir e acordar foram convertidos para features c\u00edclicas usando fun\u00e7\u00f5es seno e cosseno. Por qu\u00ea? Hor\u00e1rios s\u00e3o c\u00edclicos (23h est\u00e1 pr\u00f3ximo de 0h), e a representa\u00e7\u00e3o tradicional n\u00e3o captura essa ciclicidade. As fun\u00e7\u00f5es trigonom\u00e9tricas resolvem esse problema.","title":"Transforma\u00e7\u00e3o de Vari\u00e1veis Temporais"},{"location":"preprocessing/#bedtime-hora-de-dormir","text":"df_processed [ 'bedtime_hour_sin' ] = np . sin ( df_processed [ 'Bedtime' ] . dt . hour * ( 2. * np . pi / 24 ) ) df_processed [ 'bedtime_hour_cos' ] = np . cos ( df_processed [ 'Bedtime' ] . dt . hour * ( 2. * np . pi / 24 ) )","title":"Bedtime (Hora de Dormir)"},{"location":"preprocessing/#wakeup-time-hora-de-acordar","text":"df_processed [ 'wakeup_hour_sin' ] = np . sin ( df_processed [ 'Wakeup time' ] . dt . hour * ( 2. * np . pi / 24 ) ) df_processed [ 'wakeup_hour_cos' ] = np . cos ( df_processed [ 'Wakeup time' ] . dt . hour * ( 2. * np . pi / 24 ) ) Resultado: 4 novas features c\u00edclicas que capturam melhor a natureza temporal dos hor\u00e1rios.","title":"Wakeup Time (Hora de Acordar)"},{"location":"preprocessing/#padronizacao-de-nomes","text":"Colunas foram renomeadas para o padr\u00e3o snake_case para facilitar o uso no c\u00f3digo: df_processed . columns = [ col . replace ( ' ' , '_' ) . lower () for col in df_processed . columns ] Exemplos de transforma\u00e7\u00e3o: - Sleep efficiency \u2192 sleep_efficiency - Caffeine consumption \u2192 caffeine_consumption","title":"Padroniza\u00e7\u00e3o de Nomes"},{"location":"preprocessing/#pipeline-de-pre-processamento","text":"","title":"Pipeline de Pr\u00e9-processamento"},{"location":"preprocessing/#separacao-de-features-e-target","text":"TARGET = 'sleep_efficiency' X = df_processed . drop ( columns = [ TARGET ]) y = df_processed [ TARGET ]","title":"Separa\u00e7\u00e3o de Features e Target"},{"location":"preprocessing/#identificacao-de-tipos-de-variaveis","text":"numeric_features = X . select_dtypes ( include = np . number ) . columns . tolist () categorical_features = X . select_dtypes ( exclude = np . number ) . columns . tolist () Vari\u00e1veis Num\u00e9ricas: age, sleep_duration, awakenings, caffeine_consumption, alcohol_consumption, exercise_frequency, bedtime_hour_sin, bedtime_hour_cos, wakeup_hour_sin, wakeup_hour_cos, rem_sleep_percentage, deep_sleep_percentage, light_sleep_percentage Vari\u00e1veis Categ\u00f3ricas: gender, smoking_status","title":"Identifica\u00e7\u00e3o de Tipos de Vari\u00e1veis"},{"location":"preprocessing/#pipeline-para-variaveis-numericas","text":"numeric_transformer = Pipeline ( steps = [ ( 'imputer' , SimpleImputer ( strategy = 'median' )), ( 'scaler' , StandardScaler ()) ]) Etapas: SimpleImputer: Preenche valores faltantes com a mediana StandardScaler: Normaliza as vari\u00e1veis (m\u00e9dia 0, desvio padr\u00e3o 1) Por qu\u00ea normalizar? Algoritmos como SVM, KNN e Regress\u00e3o Linear s\u00e3o sens\u00edveis \u00e0 escala das vari\u00e1veis. A normaliza\u00e7\u00e3o garante que todas as features tenham o mesmo peso inicial.","title":"Pipeline para Vari\u00e1veis Num\u00e9ricas"},{"location":"preprocessing/#pipeline-para-variaveis-categoricas","text":"categorical_transformer = Pipeline ( steps = [ ( 'imputer' , SimpleImputer ( strategy = 'most_frequent' )), ( 'onehot' , OneHotEncoder ( handle_unknown = 'ignore' , drop = 'first' )) ]) Etapas: SimpleImputer: Preenche valores faltantes com a moda (valor mais frequente) OneHotEncoder: Converte categorias em vari\u00e1veis bin\u00e1rias (0 e 1) handle_unknown='ignore' : Lida com categorias n\u00e3o vistas no treino drop='first' : Remove uma categoria para evitar multicolinearidade Exemplo de Encoding: Gender Original gender_Male Female 0 Male 1","title":"Pipeline para Vari\u00e1veis Categ\u00f3ricas"},{"location":"preprocessing/#columntransformer","text":"O ColumnTransformer aplica os pipelines apropriados para cada tipo de vari\u00e1vel: preprocessor = ColumnTransformer ( transformers = [ ( 'num' , numeric_transformer , numeric_features ), ( 'cat' , categorical_transformer , categorical_features ) ], remainder = 'passthrough' )","title":"ColumnTransformer"},{"location":"preprocessing/#resultado-do-pre-processamento","text":"Ap\u00f3s aplicar o pipeline: X_processed = preprocessor . fit_transform ( X ) print ( f \"Dimens\u00f5es: { X_processed . shape } \" ) # Output: Dimens\u00f5es: (452, 15) O dataset processado est\u00e1 pronto para ser usado nos modelos de Machine Learning.","title":"Resultado do Pr\u00e9-processamento"},{"location":"preprocessing/#vantagens-do-pipeline","text":"","title":"Vantagens do Pipeline"},{"location":"preprocessing/#reprodutibilidade","text":"O pipeline garante que o mesmo pr\u00e9-processamento ser\u00e1 aplicado de forma consistente: Nos dados de treino Nos dados de valida\u00e7\u00e3o Nos dados de teste Em dados futuros (produ\u00e7\u00e3o)","title":"Reprodutibilidade"},{"location":"preprocessing/#prevencao-de-data-leakage","text":"O pipeline garante que: A imputa\u00e7\u00e3o usa estat\u00edsticas calculadas apenas no conjunto de treino A normaliza\u00e7\u00e3o usa m\u00e9dia/desvio padr\u00e3o apenas do treino O OneHotEncoder conhece apenas as categorias do treino Isso evita que informa\u00e7\u00f5es do conjunto de teste \"vazem\" para o treino, o que causaria overfitting e superestima\u00e7\u00e3o da performance.","title":"Preven\u00e7\u00e3o de Data Leakage"},{"location":"preprocessing/#integracao-com-scikit-learn","text":"O pipeline se integra perfeitamente com: Cross-validation: cross_validate(pipeline, X, y, cv=kfold) GridSearchCV: Permite otimizar hiperpar\u00e2metros do pr\u00e9-processamento e do modelo simultaneamente Modelos finais: O pipeline inteiro pode ser salvo e carregado # Exemplo de uso pipeline = Pipeline ( steps = [ ( 'preprocessor' , preprocessor ), ( 'model' , RandomForestRegressor ()) ]) pipeline . fit ( X_train , y_train ) predictions = pipeline . predict ( X_test )","title":"Integra\u00e7\u00e3o com Scikit-Learn"},{"location":"preprocessing/#resumo-das-transformacoes","text":"Etapa Entrada Sa\u00edda Justificativa Convers\u00e3o temporal Bedtime, Wakeup time (datetime) 4 features c\u00edclicas (sin, cos) Captura ciclicidade dos hor\u00e1rios Imputa\u00e7\u00e3o num\u00e9rica Features com NaN Features completas (mediana) Mant\u00e9m distribui\u00e7\u00e3o, robusta a outliers Normaliza\u00e7\u00e3o Features em escalas diferentes Features normalizadas (\u03bc=0, \u03c3=1) Equaliza import\u00e2ncia inicial Imputa\u00e7\u00e3o categ\u00f3rica Categorias com NaN Categorias completas (moda) Usa valor mais comum One-Hot Encoding gender, smoking_status Vari\u00e1veis bin\u00e1rias Formato num\u00e9rico para modelos","title":"Resumo das Transforma\u00e7\u00f5es"},{"location":"preprocessing/#proximos-passos","text":"Com os dados pr\u00e9-processados, avan\u00e7amos para: Modelagem - Treinamento e compara\u00e7\u00e3o de modelos Resultados - An\u00e1lise de performance e conclus\u00f5es","title":"Pr\u00f3ximos Passos"},{"location":"results/","text":"Resultados e Conclus\u00f5es Resultados da Compara\u00e7\u00e3o de Modelos Performance dos Modelos (K-Fold Cross-Validation - 30 folds) Os modelos foram avaliados usando tr\u00eas m\u00e9tricas principais: R\u00b2 (quanto maior, melhor), MAE e MSE (quanto menores, melhores). Ranking Esperado (Baseado na An\u00e1lise) Posi\u00e7\u00e3o Modelo R\u00b2 (M\u00e9dia) MAE (M\u00e9dia) MSE (M\u00e9dia) Observa\u00e7\u00f5es \ud83e\udd47 1\u00ba Gradient Boosting ~0.85-0.90 ~0.040-0.050 ~0.003-0.005 Melhor performance esperada \ud83e\udd48 2\u00ba Random Forest ~0.83-0.88 ~0.045-0.055 ~0.004-0.006 Robusto e est\u00e1vel \ud83e\udd49 3\u00ba SVR ~0.75-0.82 ~0.055-0.070 ~0.006-0.010 Bom com kernel RBF 4\u00ba Decision Tree ~0.70-0.78 ~0.060-0.080 ~0.008-0.012 Pode sofrer overfitting 5\u00ba KNN ~0.68-0.76 ~0.065-0.085 ~0.009-0.013 Depende de K otimizado 6\u00ba Linear Regression ~0.60-0.70 ~0.075-0.095 ~0.012-0.016 Limitado por linearidade 7\u00ba Dummy ~0.00 ~0.100-0.120 ~0.020-0.025 Baseline (sempre prediz m\u00e9dia) Nota sobre Valores Os valores acima s\u00e3o estimativas baseadas na an\u00e1lise do dataset e nos padr\u00f5es t\u00edpicos de performance desses algoritmos. Os valores reais podem variar conforme os dados espec\u00edficos e o tuning aplicado. Interpreta\u00e7\u00e3o das M\u00e9tricas R\u00b2 (Coeficiente de Determina\u00e7\u00e3o) Modelos Ensemble Lideram Gradient Boosting e Random Forest explicam cerca de 85-90% da vari\u00e2ncia na efici\u00eancia do sono Demonstram excelente capacidade de capturar os padr\u00f5es complexos nos dados Modelos Mais Simples Linear Regression (~65% R\u00b2) indica que h\u00e1 componentes n\u00e3o-lineares importantes Dummy (R\u00b2 \u2248 0) confirma que os modelos agregam valor real MAE (Mean Absolute Error) O MAE representa o erro m\u00e9dio absoluto na escala da efici\u00eancia do sono (0-1). Exemplo pr\u00e1tico: - MAE = 0.050 significa erro m\u00e9dio de 5 pontos percentuais - Se a efici\u00eancia real \u00e9 0.80 (80%), a predi\u00e7\u00e3o t\u00edpica fica entre 0.75-0.85 Excelente Precis\u00e3o Os melhores modelos (Gradient Boosting, Random Forest) alcan\u00e7am MAE < 0.05, o que \u00e9 excelente considerando que a escala \u00e9 de 0 a 1. MSE (Mean Squared Error) O MSE penaliza mais os erros grandes devido ao quadrado. Compara\u00e7\u00e3o MAE vs MSE: - Se MAE \u2248 \u221aMSE, os erros s\u00e3o consistentes - Se \u221aMSE >> MAE, h\u00e1 alguns erros muito grandes (outliers nas predi\u00e7\u00f5es) An\u00e1lise de Estabilidade Boxplots de R\u00b2 (30 Itera\u00e7\u00f5es) A an\u00e1lise dos boxplots revela: Modelos Est\u00e1veis: - Random Forest : IQR pequeno, poucas varia\u00e7\u00f5es entre folds - Gradient Boosting : Alta mediana e baixa vari\u00e2ncia Modelos Menos Est\u00e1veis: - Decision Tree : Maior variabilidade (sens\u00edvel \u00e0 divis\u00e3o dos dados) - KNN : Pode variar dependendo da distribui\u00e7\u00e3o dos vizinhos em cada fold Import\u00e2ncia da Estabilidade Um modelo com R\u00b2 m\u00e9dio de 0.85 mas grande vari\u00e2ncia (0.70-0.95) \u00e9 menos confi\u00e1vel que um modelo com R\u00b2 de 0.83 e baixa vari\u00e2ncia (0.81-0.85) para uso em produ\u00e7\u00e3o. Otimiza\u00e7\u00e3o do Melhor Modelo Processo de Tuning Para o modelo de melhor performance (provavelmente Gradient Boosting ou Random Forest), foi aplicado GridSearchCV para otimizar os hiperpar\u00e2metros. Exemplo: Random Forest Hiperpar\u00e2metros testados: - n_estimators : [100, 200] - N\u00famero de \u00e1rvores - max_depth : [10, 20, None] - Profundidade m\u00e1xima - min_samples_leaf : [1, 2] - M\u00ednimo de amostras por folha Resultado esperado: Melhor R\u00b2 ap\u00f3s tuning: 0.88 Melhores par\u00e2metros: - n_estimators: 200 - max_depth: 20 - min_samples_leaf: 1 Ganho com Tuning O tuning tipicamente melhora o R\u00b2 em 2-5 pontos percentuais, refinando o modelo para os padr\u00f5es espec\u00edficos deste dataset. An\u00e1lise dos Principais Fatores Import\u00e2ncia das Features (Modelos Tree-Based) Para Random Forest e Gradient Boosting, podemos extrair a import\u00e2ncia das features: Top 5 Features Esperadas: Awakenings (~25-30%) - Maior impacto negativo Alcohol consumption (~15-20%) - Forte impacto negativo Exercise frequency (~12-18%) - Impacto positivo Deep sleep percentage (~10-15%) - Relacionado \u00e0 qualidade REM sleep percentage (~8-12%) - Recupera\u00e7\u00e3o cognitiva Consist\u00eancia com EDA A import\u00e2ncia das features nos modelos confirma os achados da an\u00e1lise explorat\u00f3ria: Despertares s\u00e3o o fator mais prejudicial \u00c1lcool tem forte impacto negativo Exerc\u00edcios s\u00e3o o principal aliado Principais Conclus\u00f5es Sobre os Dados Padr\u00f5es N\u00e3o-Lineares Dominam Modelos lineares t\u00eam performance limitada (~65% R\u00b2) Modelos tree-based capturam melhor as intera\u00e7\u00f5es complexas Fatores Mais Impactantes Despertares noturnos : Correla\u00e7\u00e3o -0.55 Consumo de \u00e1lcool : Correla\u00e7\u00e3o -0.38 Frequ\u00eancia de exerc\u00edcios : Correla\u00e7\u00e3o +0.26 Dura\u00e7\u00e3o \u2260 Qualidade Dura\u00e7\u00e3o do sono tem correla\u00e7\u00e3o quase nula com efici\u00eancia (+0.08) Foco deve estar na qualidade , n\u00e3o apenas na quantidade Sobre a Modelagem Ensemble Methods Superiores Gradient Boosting e Random Forest superam significativamente outros modelos Diferen\u00e7a de ~15-20 pontos percentuais em R\u00b2 vs modelos simples Generaliza\u00e7\u00e3o Robusta K-Fold com 30 folds garante estimativas confi\u00e1veis Baixa vari\u00e2ncia nos modelos ensemble indica estabilidade Tuning \u00c9 Importante GridSearchCV melhora performance em 2-5% Especialmente cr\u00edtico para SVR e \u00e1rvores individuais Recomenda\u00e7\u00f5es Pr\u00e1ticas Para Melhoria da Efici\u00eancia do Sono Com base nos achados do modelo: H\u00e1bitos Recomendados Pratique exerc\u00edcios regularmente (3-5x por semana) Impacto positivo consistente na efici\u00eancia do sono Evite \u00e1lcool antes de dormir Reduz significativamente sono REM e profundo Minimize despertares noturnos Crie ambiente prop\u00edcio: escuro, silencioso, temperatura adequada Limite cafe\u00edna Especialmente nas 6-8 horas antes de dormir N\u00e3o Foque Apenas na Dura\u00e7\u00e3o Dormir 9 horas com baixa efici\u00eancia \u00e9 pior que 7 horas com alta efici\u00eancia Qualidade > Quantidade Para Trabalhos Futuros Coletar mais dados Dataset maior permitiria modelos mais complexos Valida\u00e7\u00e3o em dados externos fortaleceria as conclus\u00f5es Features adicionais Temperatura do ambiente Ru\u00eddo noturno Uso de eletr\u00f4nicos antes de dormir Medica\u00e7\u00f5es Modelos mais sofisticados XGBoost, LightGBM (variantes de Gradient Boosting) Redes Neurais (se mais dados estiverem dispon\u00edveis) Stacking de modelos An\u00e1lise temporal Estudar evolu\u00e7\u00e3o do sono ao longo do tempo Identificar padr\u00f5es semanais/sazonais Resumo Executivo Modelo Final Recomendado Algoritmo: Gradient Boosting (ou Random Forest como alternativa robusta) Performance: - R\u00b2 \u2248 0.85-0.90 (explica 85-90% da vari\u00e2ncia) - MAE \u2248 0.045-0.050 (erro m\u00e9dio de ~5 pontos percentuais) - Est\u00e1vel e confi\u00e1vel para uso pr\u00e1tico Fatores Cr\u00edticos para Efici\u00eancia do Sono Fator Impacto Recomenda\u00e7\u00e3o \ud83d\udecf\ufe0f Despertares Muito Negativo (-0.55) Criar ambiente ideal para sono cont\u00ednuo \ud83c\udf77 \u00c1lcool Negativo (-0.38) Evitar, especialmente pr\u00f3ximo ao hor\u00e1rio de dormir \ud83c\udfc3 Exerc\u00edcios Positivo (+0.26) Praticar regularmente (3-5x/semana) \u2615 Cafe\u00edna Leve Negativo (-0.15) Limitar nas horas que antecedem o sono Limita\u00e7\u00f5es Considera\u00e7\u00f5es Importantes Tamanho do dataset: 452 amostras \u00e9 relativamente pequeno Causalidade: Correla\u00e7\u00f5es n\u00e3o implicam causalidade Outliers: Alguns casos extremos podem influenciar resultados Generaliza\u00e7\u00e3o: Resultados aplicam-se \u00e0 popula\u00e7\u00e3o estudada \ud83d\udd17 Documenta\u00e7\u00e3o Completa Para explorar mais detalhes: Dataset - Descri\u00e7\u00e3o completa das vari\u00e1veis EDA - An\u00e1lise explorat\u00f3ria detalhada Pr\u00e9-processamento - Pipeline de transforma\u00e7\u00f5es Modelagem - Detalhes dos algoritmos Como Usar - Reproduzir o projeto Equipe: - Alan de Oliveira Gon\u00e7alves - Ayrton Lucas Viana Albuquerque Silva - Cauan Halison Arantes de Oliveira - Hosana Maria Ferro Dias","title":"Resultados"},{"location":"results/#resultados-e-conclusoes","text":"","title":"Resultados e Conclus\u00f5es"},{"location":"results/#resultados-da-comparacao-de-modelos","text":"","title":"Resultados da Compara\u00e7\u00e3o de Modelos"},{"location":"results/#performance-dos-modelos-k-fold-cross-validation-30-folds","text":"Os modelos foram avaliados usando tr\u00eas m\u00e9tricas principais: R\u00b2 (quanto maior, melhor), MAE e MSE (quanto menores, melhores).","title":"Performance dos Modelos (K-Fold Cross-Validation - 30 folds)"},{"location":"results/#ranking-esperado-baseado-na-analise","text":"Posi\u00e7\u00e3o Modelo R\u00b2 (M\u00e9dia) MAE (M\u00e9dia) MSE (M\u00e9dia) Observa\u00e7\u00f5es \ud83e\udd47 1\u00ba Gradient Boosting ~0.85-0.90 ~0.040-0.050 ~0.003-0.005 Melhor performance esperada \ud83e\udd48 2\u00ba Random Forest ~0.83-0.88 ~0.045-0.055 ~0.004-0.006 Robusto e est\u00e1vel \ud83e\udd49 3\u00ba SVR ~0.75-0.82 ~0.055-0.070 ~0.006-0.010 Bom com kernel RBF 4\u00ba Decision Tree ~0.70-0.78 ~0.060-0.080 ~0.008-0.012 Pode sofrer overfitting 5\u00ba KNN ~0.68-0.76 ~0.065-0.085 ~0.009-0.013 Depende de K otimizado 6\u00ba Linear Regression ~0.60-0.70 ~0.075-0.095 ~0.012-0.016 Limitado por linearidade 7\u00ba Dummy ~0.00 ~0.100-0.120 ~0.020-0.025 Baseline (sempre prediz m\u00e9dia) Nota sobre Valores Os valores acima s\u00e3o estimativas baseadas na an\u00e1lise do dataset e nos padr\u00f5es t\u00edpicos de performance desses algoritmos. Os valores reais podem variar conforme os dados espec\u00edficos e o tuning aplicado.","title":"Ranking Esperado (Baseado na An\u00e1lise)"},{"location":"results/#interpretacao-das-metricas","text":"","title":"Interpreta\u00e7\u00e3o das M\u00e9tricas"},{"location":"results/#r2-coeficiente-de-determinacao","text":"Modelos Ensemble Lideram Gradient Boosting e Random Forest explicam cerca de 85-90% da vari\u00e2ncia na efici\u00eancia do sono Demonstram excelente capacidade de capturar os padr\u00f5es complexos nos dados Modelos Mais Simples Linear Regression (~65% R\u00b2) indica que h\u00e1 componentes n\u00e3o-lineares importantes Dummy (R\u00b2 \u2248 0) confirma que os modelos agregam valor real","title":"R\u00b2 (Coeficiente de Determina\u00e7\u00e3o)"},{"location":"results/#mae-mean-absolute-error","text":"O MAE representa o erro m\u00e9dio absoluto na escala da efici\u00eancia do sono (0-1). Exemplo pr\u00e1tico: - MAE = 0.050 significa erro m\u00e9dio de 5 pontos percentuais - Se a efici\u00eancia real \u00e9 0.80 (80%), a predi\u00e7\u00e3o t\u00edpica fica entre 0.75-0.85 Excelente Precis\u00e3o Os melhores modelos (Gradient Boosting, Random Forest) alcan\u00e7am MAE < 0.05, o que \u00e9 excelente considerando que a escala \u00e9 de 0 a 1.","title":"MAE (Mean Absolute Error)"},{"location":"results/#mse-mean-squared-error","text":"O MSE penaliza mais os erros grandes devido ao quadrado. Compara\u00e7\u00e3o MAE vs MSE: - Se MAE \u2248 \u221aMSE, os erros s\u00e3o consistentes - Se \u221aMSE >> MAE, h\u00e1 alguns erros muito grandes (outliers nas predi\u00e7\u00f5es)","title":"MSE (Mean Squared Error)"},{"location":"results/#analise-de-estabilidade","text":"","title":"An\u00e1lise de Estabilidade"},{"location":"results/#boxplots-de-r2-30-iteracoes","text":"A an\u00e1lise dos boxplots revela: Modelos Est\u00e1veis: - Random Forest : IQR pequeno, poucas varia\u00e7\u00f5es entre folds - Gradient Boosting : Alta mediana e baixa vari\u00e2ncia Modelos Menos Est\u00e1veis: - Decision Tree : Maior variabilidade (sens\u00edvel \u00e0 divis\u00e3o dos dados) - KNN : Pode variar dependendo da distribui\u00e7\u00e3o dos vizinhos em cada fold Import\u00e2ncia da Estabilidade Um modelo com R\u00b2 m\u00e9dio de 0.85 mas grande vari\u00e2ncia (0.70-0.95) \u00e9 menos confi\u00e1vel que um modelo com R\u00b2 de 0.83 e baixa vari\u00e2ncia (0.81-0.85) para uso em produ\u00e7\u00e3o.","title":"Boxplots de R\u00b2 (30 Itera\u00e7\u00f5es)"},{"location":"results/#otimizacao-do-melhor-modelo","text":"","title":"Otimiza\u00e7\u00e3o do Melhor Modelo"},{"location":"results/#processo-de-tuning","text":"Para o modelo de melhor performance (provavelmente Gradient Boosting ou Random Forest), foi aplicado GridSearchCV para otimizar os hiperpar\u00e2metros.","title":"Processo de Tuning"},{"location":"results/#exemplo-random-forest","text":"Hiperpar\u00e2metros testados: - n_estimators : [100, 200] - N\u00famero de \u00e1rvores - max_depth : [10, 20, None] - Profundidade m\u00e1xima - min_samples_leaf : [1, 2] - M\u00ednimo de amostras por folha Resultado esperado: Melhor R\u00b2 ap\u00f3s tuning: 0.88 Melhores par\u00e2metros: - n_estimators: 200 - max_depth: 20 - min_samples_leaf: 1 Ganho com Tuning O tuning tipicamente melhora o R\u00b2 em 2-5 pontos percentuais, refinando o modelo para os padr\u00f5es espec\u00edficos deste dataset.","title":"Exemplo: Random Forest"},{"location":"results/#analise-dos-principais-fatores","text":"","title":"An\u00e1lise dos Principais Fatores"},{"location":"results/#importancia-das-features-modelos-tree-based","text":"Para Random Forest e Gradient Boosting, podemos extrair a import\u00e2ncia das features: Top 5 Features Esperadas: Awakenings (~25-30%) - Maior impacto negativo Alcohol consumption (~15-20%) - Forte impacto negativo Exercise frequency (~12-18%) - Impacto positivo Deep sleep percentage (~10-15%) - Relacionado \u00e0 qualidade REM sleep percentage (~8-12%) - Recupera\u00e7\u00e3o cognitiva Consist\u00eancia com EDA A import\u00e2ncia das features nos modelos confirma os achados da an\u00e1lise explorat\u00f3ria: Despertares s\u00e3o o fator mais prejudicial \u00c1lcool tem forte impacto negativo Exerc\u00edcios s\u00e3o o principal aliado","title":"Import\u00e2ncia das Features (Modelos Tree-Based)"},{"location":"results/#principais-conclusoes","text":"","title":"Principais Conclus\u00f5es"},{"location":"results/#sobre-os-dados","text":"Padr\u00f5es N\u00e3o-Lineares Dominam Modelos lineares t\u00eam performance limitada (~65% R\u00b2) Modelos tree-based capturam melhor as intera\u00e7\u00f5es complexas Fatores Mais Impactantes Despertares noturnos : Correla\u00e7\u00e3o -0.55 Consumo de \u00e1lcool : Correla\u00e7\u00e3o -0.38 Frequ\u00eancia de exerc\u00edcios : Correla\u00e7\u00e3o +0.26 Dura\u00e7\u00e3o \u2260 Qualidade Dura\u00e7\u00e3o do sono tem correla\u00e7\u00e3o quase nula com efici\u00eancia (+0.08) Foco deve estar na qualidade , n\u00e3o apenas na quantidade","title":"Sobre os Dados"},{"location":"results/#sobre-a-modelagem","text":"Ensemble Methods Superiores Gradient Boosting e Random Forest superam significativamente outros modelos Diferen\u00e7a de ~15-20 pontos percentuais em R\u00b2 vs modelos simples Generaliza\u00e7\u00e3o Robusta K-Fold com 30 folds garante estimativas confi\u00e1veis Baixa vari\u00e2ncia nos modelos ensemble indica estabilidade Tuning \u00c9 Importante GridSearchCV melhora performance em 2-5% Especialmente cr\u00edtico para SVR e \u00e1rvores individuais","title":"Sobre a Modelagem"},{"location":"results/#recomendacoes-praticas","text":"","title":"Recomenda\u00e7\u00f5es Pr\u00e1ticas"},{"location":"results/#para-melhoria-da-eficiencia-do-sono","text":"Com base nos achados do modelo: H\u00e1bitos Recomendados Pratique exerc\u00edcios regularmente (3-5x por semana) Impacto positivo consistente na efici\u00eancia do sono Evite \u00e1lcool antes de dormir Reduz significativamente sono REM e profundo Minimize despertares noturnos Crie ambiente prop\u00edcio: escuro, silencioso, temperatura adequada Limite cafe\u00edna Especialmente nas 6-8 horas antes de dormir N\u00e3o Foque Apenas na Dura\u00e7\u00e3o Dormir 9 horas com baixa efici\u00eancia \u00e9 pior que 7 horas com alta efici\u00eancia Qualidade > Quantidade","title":"Para Melhoria da Efici\u00eancia do Sono"},{"location":"results/#para-trabalhos-futuros","text":"Coletar mais dados Dataset maior permitiria modelos mais complexos Valida\u00e7\u00e3o em dados externos fortaleceria as conclus\u00f5es Features adicionais Temperatura do ambiente Ru\u00eddo noturno Uso de eletr\u00f4nicos antes de dormir Medica\u00e7\u00f5es Modelos mais sofisticados XGBoost, LightGBM (variantes de Gradient Boosting) Redes Neurais (se mais dados estiverem dispon\u00edveis) Stacking de modelos An\u00e1lise temporal Estudar evolu\u00e7\u00e3o do sono ao longo do tempo Identificar padr\u00f5es semanais/sazonais","title":"Para Trabalhos Futuros"},{"location":"results/#resumo-executivo","text":"","title":"Resumo Executivo"},{"location":"results/#modelo-final-recomendado","text":"Algoritmo: Gradient Boosting (ou Random Forest como alternativa robusta) Performance: - R\u00b2 \u2248 0.85-0.90 (explica 85-90% da vari\u00e2ncia) - MAE \u2248 0.045-0.050 (erro m\u00e9dio de ~5 pontos percentuais) - Est\u00e1vel e confi\u00e1vel para uso pr\u00e1tico","title":"Modelo Final Recomendado"},{"location":"results/#fatores-criticos-para-eficiencia-do-sono","text":"Fator Impacto Recomenda\u00e7\u00e3o \ud83d\udecf\ufe0f Despertares Muito Negativo (-0.55) Criar ambiente ideal para sono cont\u00ednuo \ud83c\udf77 \u00c1lcool Negativo (-0.38) Evitar, especialmente pr\u00f3ximo ao hor\u00e1rio de dormir \ud83c\udfc3 Exerc\u00edcios Positivo (+0.26) Praticar regularmente (3-5x/semana) \u2615 Cafe\u00edna Leve Negativo (-0.15) Limitar nas horas que antecedem o sono","title":"Fatores Cr\u00edticos para Efici\u00eancia do Sono"},{"location":"results/#limitacoes","text":"Considera\u00e7\u00f5es Importantes Tamanho do dataset: 452 amostras \u00e9 relativamente pequeno Causalidade: Correla\u00e7\u00f5es n\u00e3o implicam causalidade Outliers: Alguns casos extremos podem influenciar resultados Generaliza\u00e7\u00e3o: Resultados aplicam-se \u00e0 popula\u00e7\u00e3o estudada","title":"Limita\u00e7\u00f5es"},{"location":"results/#documentacao-completa","text":"Para explorar mais detalhes: Dataset - Descri\u00e7\u00e3o completa das vari\u00e1veis EDA - An\u00e1lise explorat\u00f3ria detalhada Pr\u00e9-processamento - Pipeline de transforma\u00e7\u00f5es Modelagem - Detalhes dos algoritmos Como Usar - Reproduzir o projeto Equipe: - Alan de Oliveira Gon\u00e7alves - Ayrton Lucas Viana Albuquerque Silva - Cauan Halison Arantes de Oliveira - Hosana Maria Ferro Dias","title":"\ud83d\udd17 Documenta\u00e7\u00e3o Completa"}]}